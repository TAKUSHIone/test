{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5407,"databundleVersionId":868283,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-13T01:32:47.949953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n#TotalSF （TotalBsmtSF + 1stFlrSF + 2ndFlrSF） の追加ように下記を作成8月２３日\ntrain_df2  = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n#行数の表示制限を解除\npd.set_option('display.max_rows', None)\n#列数の表示制限を解除\npd.set_option('display.max_columns', None)\n#表示が膨大になるので、必ず head()を設定すること。\ntrain_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 0 目的変数の分布図を確認して、正規分布によっているかどうか対数変換などで整形はいるか？ などを判断","metadata":{}},{"cell_type":"code","source":"# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(train_df['SalePrice'], kde=True)\n# plt.title('Before SalePrice plots')\n# plt.xlabel('SalePrice')\n# plt.ylabel('度数')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## train_df SalePrice の対数変換前のバックアップ","metadata":{}},{"cell_type":"code","source":"#実践的な判断フロー\n#計算: 特徴量の歪度を計算する (.skew())。\n#閾値チェック: 絶対値が 0.5 や 1.0 を超えているか確認する。\n#性質チェック: それは変換可能な「連続値」か？ (OverallQual のようなものではないか？)\n#可視化: ヒストグラムで分布の形を目で見て確認する。\n#実験: 変換した場合としない場合で、モデルの精度を比較してみる。\n#この多角的な視点を持つことが、より精度の高いモデルを作成するための鍵となります。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#明確な黄金律（ゴールデンルール）はありませんが、データサイエンスの実務でよく用いられる経験則としての一般的な目安は以下の通りです。\n#歪度の目安と対数変換の判断基準\n#歪度は、正負両方の値を取りうるため、絶対値で判断するのが一般的です。\n#| 歪度の絶対値 (|skew|) | 評価 | 対数変換の必要性 |\n#| :--- | :--- | :--- |\n#| 0.5 以下 | ほぼ対称 (Approximately Symmetric) | 原則として不要です。変換による改善効果はほとんど期待できません。 |\n#| 0.5 ～ 1.0 | 中程度に歪んでいる (Moderately Skewed) | 検討の価値あり。変換によってモデルの性能が向上する可能性があります。特に線形モデルを使う場合は有効なことが多いです。 |\n#| 1.0 より大きい | 強く歪んでいる (Highly Skewed) | 強く推奨されます。変換しないと、外れ値の影響を強く受けたり、モデルの仮定（正規性など）から外れたりして、精度が悪化する可能性が高いです。 |\n#Kaggleの住宅価格データ（train.csv）で例を見てみましょう。\n#SalePrice の歪度: 約 1.88\n#これは 1.0 を大幅に超えており、「強く歪んでいる」に該当します。そのため、対数変換やYeo-Johnson変換が非常に効果的です。\n#GrLivArea の歪度: 約 1.37\n#これも 1.0 を超えているため、変換の対象として有力な候補となります。\n#OverallQual の歪度: 約 -0.22\n#絶対値が 0.5 以下なので、「ほぼ対称」と判断できます。この数値からも、この特徴量に歪み補正は不要であることがわかります。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#結果の表示と解釈: 計算した歪度の値を表示し、その値が一般的にどのように解釈されるかの目安も表示するようにしました。\n#歪度が0に近い: 分布が左右対称（正規分布など）に近いことを意味します。\n#歪度が正の値: 分布の山が左に寄り、右側に裾が長く伸びている（右に歪んだ）分布です。平均値が中央値よりも大きくなる傾向があります。SalePrice はこの典型例です。\n#歪度が負の値: 分布の山が右に寄り、左側に裾が長く伸びている（左に歪んだ）分布です。\n\n# --- 1. 'OverallQual' の歪度を計算 ---\nskewness = train_df['SalePrice'].skew()\n\n\n# --- 2. 結果の表示 ---\nprint(f\" SalePriceの歪度: {skewness:.4f}\")\nprint(\"-----------------------------------------\")\n\n\n# --- 歪度の解釈 ---\nprint(\"【歪度の目安】\")\nif -0.5 <= skewness <= 0.5:\n    print(\" -> ほぼ左右対称な分布です。\")\nelif skewness > 0.5:\n    print(\" -> 右に裾が長い（右に歪んだ）分布です。\")\nelse: # skewness < -0.5\n    print(\" -> 左に裾が長い（左に歪んだ）分布です。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1460ID（行）と、81カラム（列）とわかる。\ntrain_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1460IDの（行）が、1460とわかる。\nlen(train_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 見かけだけでカラムのデータを判断しないように、まずはカラムの種類を調べる","metadata":{}},{"cell_type":"code","source":"#データフレームの中身の種類を確認するときは、必ずすべてを確認するために、\n#行数の表示制限を解除　pd.set_option('display.max_rows', None)\n#列数の表示制限を解除　pd.set_option('display.max_columns', None)\n#の表示制限解除が必要となる場合がある。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 欠損値の確認","metadata":{}},{"cell_type":"code","source":"#ベースライン用に、まずは欠損値がないものを探す。\nprint(train_df.isnull().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 特徴量のデータ型の確認","metadata":{}},{"cell_type":"code","source":"#欠損値がないかつカラムのデータ型を確認する。（RMSEの場合、object型は扱えない)\ntrain_df.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 学習用データ全体とテストデータ全体のバックアップ","metadata":{}},{"cell_type":"code","source":"#testデータの読み込みと。ID数とカラム数を表示をさせている\ntest = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n#TotalSF （TotalBsmtSF + 1stFlrSF + 2ndFlrSF） の追加用で下記を作成した。\ntest2 = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\ntest.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_backup = train_df.copy()\ntest_backup = test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 特徴量の選択","metadata":{}},{"cell_type":"code","source":"#以下、あたりをつけた特徴量\nfeatures=[\"SaleCondition\", \"FullBath\", \"SaleType\", \"Functional\", \"MSZoning\", \"Neighborhood\", \"LotArea\", \"MSSubClass\", \"TotRmsAbvGrd\", \"GrLivArea\", \"1stFlrSF\", \"OverallQual\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinSF1\", \"HeatingQC\", \"CentralAir\", \"KitchenAbvGr\", \"KitchenQual\", \"ExterQual\", \"GarageCars\", \"GarageArea\", \"OverallCond\"]\n#目的変数y_train と説明変数x_trainに分けた。ここは、まだベースラインなのでドメイン知識と欠損値がないという理由だけの説明変数の選択。\nx_train = train_df[features]\ny_train = train_df[\"SalePrice\"]\n\nprint(x_train.shape)\nprint(y_train.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### featuresのバックアップ","metadata":{}},{"cell_type":"code","source":"# featuresのバックアップOverallQualの追加とHeatingの削除\n#↓\n#features_back_up_0810 = features \n#FullBath の追加\n#↓\n#features_back_up_0812 = features\n\n#YearBuilt、YrSold の追加\n#↓\n#features_back_up_0813 = features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"features","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 特徴量選択後のテストデータの定義","metadata":{}},{"cell_type":"code","source":"#X_test を事前に読み込んだ特徴量の数（features)と設定し、表示させている。\n\nx_test = test[features]\nx_test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 特徴量選択後の学習用データとテストデータのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup = x_train.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 特徴量 の順序付きマッピング ＠＠最初に欠損値と種類はデータフレームを作成して一覧できる！とにかく時間を短縮できそうならする！","metadata":{}},{"cell_type":"markdown","source":"#### 特徴量選択後の学習用データの欠損値を調べる。","metadata":{}},{"cell_type":"code","source":"#データフレームで、選択した特徴量の欠損件数等をあらかじめ表示させる\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 特徴量選択後のテストデータの欠損値を調べる。（欠損値への処理は学習用、テスト用ともに必要なので、かならずどちらも欠損値を確認する）","metadata":{}},{"cell_type":"code","source":"# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 選択した特徴量の種類とそのサンプル値を確認する。（学習用データ）","metadata":{}},{"cell_type":"code","source":"# #pandas のデフォルト表示幅制限を解除↓\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n    \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 選択した特徴量の種類とそのサンプル値を確認する。（テストデータ）","metadata":{}},{"cell_type":"code","source":"# #pandas のデフォルト表示幅制限を解除↓\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n# }, index=features)\n\n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Heating QC のマッピング","metadata":{}},{"cell_type":"code","source":"# ##学習用データの型と値の種類を確認する\n# print(x_train['HeatingQC'].dtype)\n# print(x_train['HeatingQC'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #テスト用データの型と値の種類を確認する\n# print(x_test['HeatingQC'].dtype)\n# print(x_test['HeatingQC'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"heatingQC_order = {\n    'Ex': 4,\n    'Gd': 3,\n    'TA': 2,\n    'Fa': 1,  \n    \"Po\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['HeatingQC'] = x_train['HeatingQC'].map(heatingQC_order)\nx_test['HeatingQC'] = x_test['HeatingQC'].map(heatingQC_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['HeatingQC'].isna().sum()\n# print(f\"'HeatingQC' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['HeatingQC'].isna().sum()\n# print(f\"'HeatingQC' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### CentralAir\tのマッピング","metadata":{}},{"cell_type":"code","source":"# ##学習用データの型と値の種類を確認する\n# print(x_train['CentralAir'].dtype)\n# print(x_train['CentralAir'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##テスト用データの型と値の種類を確認する\n# print(x_test['CentralAir'].dtype)\n# print(x_test['CentralAir'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"CentralAir_order = {\n    'Y': 1,\n    'N': 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['CentralAir'] = x_train['CentralAir'].map(CentralAir_order)\nx_test['CentralAir'] = x_test['CentralAir'].map(CentralAir_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['CentralAir'].isna().sum()\n# print(f\"'CentralAir' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['CentralAir'].isna().sum()\n# print(f\"'CentralAir' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Heating,Heating QC ,CentralAir のマッピング後のバックアップ(0709)","metadata":{}},{"cell_type":"code","source":"# x_train_backup0709 = x_train.copy()\n# x_test_backup0709 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### BsmtCondのマッピング（欠損値 train,test ともに欠損値あり）欠損値を補完したあとでないと、型の変換はできない","metadata":{}},{"cell_type":"code","source":"# ##学習用データの型と値の種類を確認する\n# print(x_train['BsmtCond'].dtype)\n# print(x_train['BsmtCond'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##テスト用データの型と値の種類を確認する\n# print(x_test['BsmtCond'].dtype)\n# print(x_test['BsmtCond'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BsmtCond\n#[TA, Po, Fa, Gd]\n\nBsmtCond_order = {\n    'Gd': 3,\n    'TA': 2,\n    'Fa': 1,  \n    \"Po\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['BsmtCond'] = x_train['BsmtCond'].map(BsmtCond_order)\nx_test['BsmtCond'] = x_test['BsmtCond'].map(BsmtCond_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['BsmtCond'].isna().sum()\n# print(f\"'BsmtCond' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['BsmtCond'].isna().sum()\n# print(f\"'BsmtCond' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Heating,Heating QC,CentralAir,BsmtCond のマッピング後のバックアップ(0709_1)¶","metadata":{}},{"cell_type":"code","source":"# x_train_backup0709_1 = x_train.copy()\n# x_test_backup0709_1 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### KitchenAbvGrのマッピングint 型なので、キーとマップをintでそろえること","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['KitchenAbvGr'].dtype)\n# print(x_train['KitchenAbvGr'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test['KitchenAbvGr'].dtype)\n# print(x_test['KitchenAbvGr'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"KitchenAbvGr_order = {\n     1: '2',\n     2: '1',\n     3: '1',  \n     0: '0',\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['KitchenAbvGr'] = x_train['KitchenAbvGr'].map(KitchenAbvGr_order)\nx_test['KitchenAbvGr'] = x_test['KitchenAbvGr'].map(KitchenAbvGr_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['KitchenAbvGr'].isna().sum()\n# print(f\"'KitchenAbvGr' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['KitchenAbvGr'].isna().sum()\n# print(f\"'KitchenAbvGr' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#int へ戻した。\nx_train['KitchenAbvGr'] = x_train['KitchenAbvGr'].astype(int)\nx_test['KitchenAbvGr'] = x_test['KitchenAbvGr'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Heating, Heating QC, CentralAir, BsmtCond, KitchenAbvGr のマッピング後のバックアップ(0710)¶","metadata":{}},{"cell_type":"code","source":"# x_train_backup0710 = x_train.copy()\n# x_test_backup0710 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### KitchenQualのマッピング (欠損値TESTデータに１あり）","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['KitchenQual'].dtype)\n# print(x_train['KitchenQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テストデータの型と値の種類を確認する\n# print(x_test['KitchenQual'].dtype)\n# print(x_test['KitchenQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#KitchenQual\n#TA, Gd, Ex, Fa\n\nKitchenQual_order = {\n    'Ex': 4,\n    'Gd': 3,\n    'TA': 2,  \n    \"Fa\": 1,\n    \"Po\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['KitchenQual'] = x_train['KitchenQual'].map(KitchenQual_order)\nx_test['KitchenQual'] = x_test['KitchenQual'].map(KitchenQual_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['KitchenQual'].isna().sum()\n# print(f\"'KitchenQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['KitchenQual'].isna().sum()\n# print(f\"'KitchenQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup0711 = x_train.copy()\n# x_test_backup0711 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ExterQualのマッピング欠損値なし","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['ExterQual'].dtype)\n# print(x_train['ExterQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test['ExterQual'].dtype)\n# print(x_test['ExterQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ExterQual\t4\t[TA, Gd, Ex, Fa]\nExterQual_order = {\n    'Ex': 3,\n    'Gd': 2,\n    'TA': 1,  \n    \"Fa\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['ExterQual'] = x_train['ExterQual'].map(ExterQual_order)\nx_test['ExterQual'] = x_test['ExterQual'].map(ExterQual_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['ExterQual'].isna().sum()\n# print(f\"'ExterQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['ExterQual'].isna().sum()\n# print(f\"'ExterQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup0711_1 = x_train.copy()\n# x_test_backup0711_1 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### BsmtQualのマッピング（欠損値　学習用、テスト共に３０以上あり）","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['BsmtQual'].dtype)\n# print(x_train['BsmtQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test['BsmtQual'].dtype)\n# print(x_test['BsmtQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BsmtQual\t4\t[TA, Gd, Ex, Fa]\n\nBsmtQual_order = {\n    'Ex': 3,\n    'Gd': 2,\n    'TA': 1,  \n    \"Fa\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['BsmtQual'] = x_train['BsmtQual'].map(BsmtQual_order)\nx_test['BsmtQual'] = x_test['BsmtQual'].map(BsmtQual_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['BsmtQual'].isna().sum()\n# print(f\"'BsmtQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['BsmtQual'].isna().sum()\n# print(f\"'BsmtQual' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup0711_2 = x_train.copy()\n# x_test_backup0711_2 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### BsmtExposure　のマッピング（学習用、テスト用ともに欠損値３０以上あり）","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['BsmtExposure'].dtype)\n# print(x_train['BsmtExposure'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test['BsmtExposure'].dtype)\n# print(x_test['BsmtExposure'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BsmtExposure\t4\t[No, Gd, Mn, Av]\n\nBsmtExposure_order = {\n    'Gd': 3,\n    'Av': 2,\n    'Mn': 1,  \n    \"No\": 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['BsmtExposure'] = x_train['BsmtExposure'].map(BsmtExposure_order)\nx_test['BsmtExposure'] = x_test['BsmtExposure'].map(BsmtExposure_order)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['BsmtExposure'].isna().sum()\n# print(f\"'BsmtExposure' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['BsmtExposure'].isna().sum()\n# print(f\"'BsmtExposure' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup0711_3 = x_train.copy()\n# x_test_backup0711_3 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### GarageCars のマッピング（テストデータに１欠損値あり）trainの　型int testの型float かつtrainに欠損値ないため、trainをfloat にしてからマッピング予定","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['GarageCars'].dtype)\n# print(x_train['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test['GarageCars'].dtype)\n# print(x_test['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 上記intとfloat のアンマッチがあるため、trainに欠損値ないため、int→float にキャストしてから、マッピングし、ガレージ数5の種類を活かすこととした。","metadata":{}},{"cell_type":"code","source":"#学習用データの'GarageCarsをfloatへ変更。なお、float の場合は、欠損値NaNがあってもキャストを許容できる。ため、fillnaが不要。\nx_train['GarageCars'] = x_train['GarageCars'].astype(float)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### GarageCarsはfloat になっているが、種類の表記は必ずデータフレームで確認すること。（データフレームは1.0 2.0 の表記である）","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['GarageCars'].dtype)\n# print(x_train['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test['GarageCars'].dtype)\n# print(x_test['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#左がラベル（ここでは数字）　右が意味を持たせる数字\n#テストデータ[1.0, 2.0, 3.0, 0.0, 4.0, 5.0]\n#学習用データ[2.0, 3.0, 1.0, 0.0, 4.0]\n#上記のように、テストデータの5.0にも意味付けをしないと、アンマッチをおこす。\n#テストデータに np.nan（欠損値）が含まれていた #→ NaN を含む列は 自動的に float64 型にされます（Pandasの仕様）。\n#1. 多くの機械学習モデル（特にツリー系）は、型に鈍感\n#float64 も int64 も、数値としての大小関係が保たれていれば問題なし。\n#例：LightGBM・XGBoost・RandomForest などは、型より「数値の順序」を見ます。\n#ただし、「型の違いが原因でエラーになる」場合もある（主に以下）\n#ケース\t影響\n#One-Hot Encoding や TargetEncoding に型の差がある\tエラーまたは不一致の原因に\n#pandas の処理（結合・比較）で型が不一致\tmerge や groupby でエラーの可能性\n#明示的な型チェックやモデル制限がある場合\t特定のライブラリ（e.g. CatBoost）では警告も\n\nGarageCars_order = {\n     5.0: 5,\n     4.0: 4,\n     3.0: 3,  \n     2.0: 2,\n     1.0: 1,\n     0.0: 0,   \n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['GarageCars'] = x_train['GarageCars'].map(GarageCars_order)\nx_test['GarageCars'] = x_test['GarageCars'].map(GarageCars_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['GarageCars'].isna().sum()\n# print(f\"'GarageCars' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['GarageCars'].isna().sum()\n# print(f\"'GarageCars' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['GarageCars'].dtype)\n# print(x_train['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#x_test に欠損値があるため、astype(int)はできない状態であるため、学習及びテストデータの型をfloat にそろえるため以下を実行\nx_train['GarageCars'] = x_train['GarageCars'].astype(float)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['GarageCars'].dtype)\n# print(x_train['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test['GarageCars'].dtype)\n# print(x_test['GarageCars'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでの最新のバックアップ↓","metadata":{}},{"cell_type":"code","source":"# x_train_backup0713 = x_train.copy()\n# x_test_backup0713 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 標準化、正規化するにあたっての注意点","metadata":{}},{"cell_type":"code","source":"#比較項目\t標準化（Standardization）\t正規化（Normalization）\n #単位\t          各特徴量（列）ごと\t    各サンプル（行）ごと\n #効果\t          平均0・標準偏差1に\t    ベクトル長を1に\n #用途\t          線形回帰・SVMなど\t    KNN・NNなど\n #外れ値の影響\t  大きい\t                小さい","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#StandardScaler は、外れ値の影響を受けやすいので、よく外れ値を確認しておくこと。（のちの外れ値対処に資するため）","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### LotAreaの標準化","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup0713['LotArea'].dtype)\n# print(x_train_backup0713['LotArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test_backup0713['LotArea'].dtype)\n# print(x_test_backup0713['LotArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['LotArea'].quantile(0.25)\n# q3 = x_train_backup['LotArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['LotArea'] < lower) | (x_train_backup['LotArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='LotArea', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"LotArea Outlier Visualization of LotArea\")\n# plt.ylabel('LotArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['LotArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['LotArea'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['LotArea'].quantile(0.25)\n# q3 = x_test_backup['LotArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['LotArea'] < lower) | (x_test_backup['LotArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='LotArea', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"LotArea Outlier Visualization of LotArea\")\n# plt.ylabel('LotArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テスト用データの平均、最大、最小を確認\n# x_test_backup['LotArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['LotArea'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 標準化の例","metadata":{}},{"cell_type":"code","source":"#LotArea に標準化が適している理由\n#LotArea は数千〜数万と値のスケールが大きくばらつきも大きい\n\n#外れ値（例：100000㎡など）も混在することが多い\n\n#正規化（0～1）は外れ値のせいでほとんどの値が0に寄ってしまうため、情報がつぶれやすい\n\n#標準化なら平均と標準偏差を使うため、外れ値がいても比較的ロバストに処理できる\n\n#入力が int 型でも問題なし\t自動で float に変換されるためエラーにはならない\n#出力は float64 型になる\t小数点の値になるため、型変換を気にする場合は注意\n#元に戻したい場合\tinverse_transform() で元のスケールに戻せる","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### standardscaler は外れ値の影響を受けやすいため、以下RobustScalerで標準化","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['LotArea'] = scaler.fit_transform(x_train[['LotArea']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['LotArea'] = scaler.transform(x_test[['LotArea']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['LotArea'].isna().sum()\n# print(f\"'LotArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['LotArea'].isna().sum()\n# print(f\"'LotArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0716 = x_train.copy()\n# x_test_backup0716 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"GrLivArea の標準化","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['GrLivArea'].dtype)\n# print(x_train_backup['GrLivArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test_backup['GrLivArea'].dtype)\n# print(x_test_backup['GrLivArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['GrLivArea'].quantile(0.25)\n# q3 = x_train_backup['GrLivArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['GrLivArea'] < lower) | (x_train_backup['GrLivArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='GrLivArea', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"GrLivArea Outlier Visualization of GrLivArea\")\n# plt.ylabel('GrLivArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['GrLivArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['GrLivArea'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['GrLivArea'].quantile(0.25)\n# q3 = x_test_backup['GrLivArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['GrLivArea'] < lower) | (x_test_backup['GrLivArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='GrLivArea', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"GrLivArea Outlier Visualization of GrLivArea\")\n# plt.ylabel('GrLivArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #テストデータの平均、最大、最小を確認\n# x_test_backup['GrLivArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['GrLivArea'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['GrLivArea'] = scaler.fit_transform(x_train[['GrLivArea']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['GrLivArea'] = scaler.transform(x_test[['GrLivArea']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['GrLivArea'].isna().sum()\n# print(f\"'GrLivArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['GrLivArea'].isna().sum()\n# print(f\"'GrLivArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0717 = x_train.copy()\n# x_test_backup0717 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### '1stFlrSF'の標準化","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['1stFlrSF'].dtype)\n# print(x_train_backup['1stFlrSF'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test_backup['1stFlrSF'].dtype)\n# print(x_test_backup['1stFlrSF'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['1stFlrSF'].quantile(0.25)\n# q3 = x_train_backup['1stFlrSF'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['1stFlrSF'] < lower) | (x_train_backup['1stFlrSF'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='1stFlrSF', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"1stFlrSF Outlier Visualization of 1stFlrSF\")\n# plt.ylabel('1stFlrSF')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['1stFlrSF'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['1stFlrSF'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['1stFlrSF'].quantile(0.25)\n# q3 = x_test_backup['1stFlrSF'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['1stFlrSF'] < lower) | (x_test_backup['1stFlrSF'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='1stFlrSF', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"1stFlrSF Outlier Visualization of 1stFlrSF\")\n# plt.ylabel('1stFlrSF')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_test_backup['1stFlrSF'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['1stFlrSF'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#1stFlrSF 欠損値なし。外れ値もほぼなさそう。\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['1stFlrSF'] = scaler.fit_transform(x_train[['1stFlrSF']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['1stFlrSF'] = scaler.transform(x_test[['1stFlrSF']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['1stFlrSF'].isna().sum()\n# print(f\"'1stFlrSF' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['1stFlrSF'].isna().sum()\n# print(f\"'1stFlrSF' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0722 = x_train.copy()\n# x_test_backup0722 = x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 'BsmtFinSF1の標準化","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['BsmtFinSF1'].dtype)\n# print(x_train_backup['BsmtFinSF1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test_backup['BsmtFinSF1'].dtype)\n# print(x_test_backup['BsmtFinSF1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['BsmtFinSF1'].quantile(0.25)\n# q3 = x_train_backup['BsmtFinSF1'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['BsmtFinSF1'] < lower) | (x_train_backup['BsmtFinSF1'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='BsmtFinSF1', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"BsmtFinSF1 Outlier Visualization of BsmtFinSF1\")\n# plt.ylabel('BsmtFinSF1')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['BsmtFinSF1'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['BsmtFinSF1'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['BsmtFinSF1'].quantile(0.25)\n# q3 = x_test_backup['BsmtFinSF1'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['BsmtFinSF1'] < lower) | (x_test_backup['BsmtFinSF1'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='BsmtFinSF1', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"BsmtFinSF1 Outlier Visualization of BsmtFinSF1\")\n# plt.ylabel('BsmtFinSF1')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #テスト用データの平均、最大、最小を確認\n# x_test_backup['BsmtFinSF1'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['BsmtFinSF1'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BsmtFinSF1 TESTデータに欠損値１あり。外れ値あり。\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['BsmtFinSF1'] = scaler.fit_transform(x_train[['BsmtFinSF1']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['BsmtFinSF1'] = scaler.transform(x_test[['BsmtFinSF1']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['BsmtFinSF1'].isna().sum()\n# print(f\"'BsmtFinSF1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['BsmtFinSF1'].isna().sum()\n# print(f\"'BsmtFinSF1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0722_1 = x_train.copy()\n# x_test_backup0722_1= x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### GarageAreaの標準化","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['GarageArea'].dtype)\n# print(x_train_backup['GarageArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###TEST用データの型と値の種類を確認する\n# print(x_test_backup['GarageArea'].dtype)\n# print(x_test_backup['GarageArea'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['GarageArea'].quantile(0.25)\n# q3 = x_train_backup['GarageArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['GarageArea'] < lower) | (x_train_backup['GarageArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='GarageArea', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"GarageArea Outlier Visualization of GarageArea\")\n# plt.ylabel('GarageArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['GarageArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['GarageArea'].value_counts().sort_index().head().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['GarageArea'].quantile(0.25)\n# q3 = x_test_backup['GarageArea'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['GarageArea'] < lower) | (x_test_backup['GarageArea'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='GarageArea', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"GarageArea Outlier Visualization of GarageArea\")\n# plt.ylabel('GarageArea')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #テスト用データの平均、最大、最小を確認\n# x_test_backup['GarageArea'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['GarageArea'].value_counts().sort_index().head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#GarageArea TESTデータに欠損値１あり。外れ値あり。\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['GarageArea'] = scaler.fit_transform(x_train[['GarageArea']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['GarageArea'] = scaler.transform(x_test[['GarageArea']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['GarageArea'].isna().sum()\n# print(f\"'GarageArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['GarageArea'].isna().sum()\n# print(f\"'GarageArea' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup0722_2 = x_train.copy()\n# x_test_backup0722_2= x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### TotRmsAbvGrdの調査　欠損値なし　trainとtest ともにfloat64","metadata":{}},{"cell_type":"markdown","source":"##### 標準化するより試してみたい前処理（場合により有効）\n##### 手法\t目的\n##### カテゴリビニング（例：小・中・大）\tモデルに非線形効果を明示（価格は一定数超で跳ねる等）\n##### 相互作用特徴量（TotRmsAbvGrd ÷ BedroomAbvGr）\t平均部屋サイズの proxy\n##### 居住面積あたりの部屋数（TotRmsAbvGrd / GrLivArea）\t間取り効率を表現\n##### ただし、他とのスケール差大であれば影響はするので、標準化候補とはなる。","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['TotRmsAbvGrd'].dtype)\n# print(x_train_backup['TotRmsAbvGrd'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###テスト用データの型と値の種類を確認する\n# print(x_test_backup['TotRmsAbvGrd'].dtype)\n# print(x_test_backup['TotRmsAbvGrd'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train_backup['TotRmsAbvGrd'].quantile(0.25)\n# q3 = x_train_backup['TotRmsAbvGrd'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train_backup['Outlier_Flag'] = ((x_train_backup['TotRmsAbvGrd'] < lower) | (x_train_backup['TotRmsAbvGrd'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train_backup.index, y='TotRmsAbvGrd', hue='Outlier_Flag', data=x_train_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"TotRmsAbvGrd Outlier Visualization of TotRmsAbvGrd\")\n# plt.ylabel('TotRmsAbvGrd')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train_backup['TotRmsAbvGrd'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup['TotRmsAbvGrd'].value_counts().sort_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テストデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test_backup['TotRmsAbvGrd'].quantile(0.25)\n# q3 = x_test_backup['TotRmsAbvGrd'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test_backup['Outlier_Flag'] = ((x_test_backup['TotRmsAbvGrd'] < lower) | (x_test_backup['TotRmsAbvGrd'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test_backup.index, y='TotRmsAbvGrd', hue='Outlier_Flag', data=x_test_backup, palette={False: 'blue', True: 'red'})\n# plt.title(\"TotRmsAbvGrd Outlier Visualization of TotRmsAbvGrd\")\n# plt.ylabel('TotRmsAbvGrd')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_test_backup['TotRmsAbvGrd'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup['TotRmsAbvGrd'].value_counts().sort_index()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['TotRmsAbvGrd'].isna().sum()\n# print(f\"'TotRmsAbvGrd' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['TotRmsAbvGrd'].isna().sum()\n# print(f\"'TotRmsAbvGrd' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0723 = x_train.copy()\n# x_test_backup0723= x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### BsmtFinType1 のマッピング（テストデータ、学習データともに欠損値３０以上あり）","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['BsmtFinType1'].dtype)\n# print(x_train_backup['BsmtFinType1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train_backup['BsmtFinType1'].isna().sum()\n# print(f\"'BsmtFinType1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###testデータの型と値の種類を確認する\n# print(x_test_backup['BsmtFinType1'].dtype)\n# print(x_test_backup['BsmtFinType1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test_backup['BsmtFinType1'].isna().sum()\n# print(f\"'BsmtFinType1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#BsmtFinType1\n#[GLQ' 'ALQ' 'Unf' 'Rec' 'BLQ' nan 'LwQ]\n\nBsmtFinType1_order = {\n    'GLQ': 5,\n    'ALQ': 4,\n    'BLQ': 3,  \n    'Rec': 2,\n    'Lwq': 1,\n    'Unf': 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['BsmtFinType1'] = x_train['BsmtFinType1'].map(BsmtFinType1_order)\nx_test['BsmtFinType1'] = x_test['BsmtFinType1'].map(BsmtFinType1_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #欠損値となしの数が合算されている。本当の欠損値となし　を区分けする必要あり。\n# missing_count = x_train['BsmtFinType1'].isna().sum()\n# print(f\"'BsmtFinType1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #欠損値となしの数が合算されている。#欠損値となしの数が合算されている。\n# missing_count = x_test['BsmtFinType1'].isna().sum()\n# print(f\"'BsmtFinType1' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['BsmtFinType1'].dtype)\n# print(x_train['BsmtFinType1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_test['BsmtFinType1'].dtype)\n# print(x_test['BsmtFinType1'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['BsmtFinType1'].quantile(0.25)\n# q3 = x_train['BsmtFinType1'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['BsmtFinType1'] < lower) | (x_train['BsmtFinType1'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='BsmtFinType1', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"BsmtFinType1 Outlier Visualization of BsmtFinType1\")\n# plt.ylabel('BsmtFinType1')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #学習用データの平均、最大、最小を確認\n# x_train['BsmtFinType1'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#testデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['BsmtFinType1'].quantile(0.25)\n# q3 = x_test['BsmtFinType1'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['BsmtFinType1'] < lower) | (x_test['BsmtFinType1'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='BsmtFinType1', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"BsmtFinType1 Outlier Visualization of BsmtFinType1\")\n# plt.ylabel('BsmtFinType1')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #testデータの平均、最大、最小を確認\n# x_test['BsmtFinType1'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Outlier_Flag をカラムから消した。\nx_train = x_train.drop(columns=['Outlier_Flag'], errors='ignore')\nx_test = x_test.drop(columns=['Outlier_Flag'], errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### ここまでのバックアップ","metadata":{}},{"cell_type":"code","source":"# x_train_backup0724 = x_train.copy()\n# x_test_backup0725= x_test.copy()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# マッピング例（例＊Functional train,test 共にobject 欠損値testに２あり）の手順","metadata":{}},{"cell_type":"markdown","source":"#### Functionalのマッピング　テストデータに２の欠損値あり","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #テスト用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['Functional'].dtype)\n# print(x_train_backup['Functional'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###テストデータの型と値の種類を確認する\n# print(x_test_backup['Functional'].dtype)\n# print(x_test_backup['Functional'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #テスト用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのFunctionalの欠損件数を個別に確認する。\n# missing_count = x_train_backup['Functional'].isna().sum()\n# print(f\"'Functional' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###testデータのFunctionalの欠損件数を個別に確認する。\n# missing_count = x_test_backup['Functional'].isna().sum()\n# print(f\"'Functional' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#9\n#Functional マッピング　テストデータに２の欠損値であり、型はobject であることに留意\n#'Typ' 'Min1' 'Maj1' 'Min2' 'Mod' 'Maj2' 'Sev'左記は学習用データ\n#['Typ' 'Min2' 'Min1' 'Mod' 'Maj1' 'Sev' 'Maj2' nan]  左記はテストデータ\n\nFunctional_order = {\n    'Typ': 6,\n    'Min1': 5,\n    'Min2': 4,\n    'Mod': 3,\n    'Maj1': 2,  \n    'Maj2': 1,\n    'Sev': 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['Functional'] = x_train['Functional'].map(Functional_order)\nx_test['Functional'] = x_test['Functional'].map(Functional_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Functionalのマッピング後のデータ内容の確認","metadata":{}},{"cell_type":"code","source":"# #10\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_train['Functional'].isna().sum()\n# print(f\"'Functional' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #11\n# ###マッピング後のテスト用データの型と値の種類を確認する\n# missing_count = x_test['Functional'].isna().sum()\n# print(f\"'Functional' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#### 上記から、マッピング後に欠損値の増加等がないことがわかる。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['Functional'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# #マッピング後のテスト用データの平均、最大、最小を確認\n# x_test['Functional'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#14\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['Functional'].quantile(0.25)\n# q3 = x_train['Functional'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['Functional'] < lower) | (x_train['Functional'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='Functional', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"Functional Outlier Visualization of Functional\")\n# plt.ylabel('Functional')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#15\n#マッピング後のtestデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['Functional'].quantile(0.25)\n# q3 = x_test['Functional'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['Functional'] < lower) | (x_test['Functional'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='Functional', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"Functional Outlier Visualization of Functional\")\n# plt.ylabel('Functional')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SaleConditionのマッピング","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #テスト用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['SaleCondition'].dtype)\n# print(x_train_backup['SaleCondition'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###test用データの型と値の種類を確認する\n# print(x_test_backup['SaleCondition'].dtype)\n# print(x_test_backup['SaleCondition'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #test用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_train_backup['SaleCondition'].isna().sum()\n# print(f\"'Functional' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###test用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_test_backup['SaleCondition'].isna().sum()\n# print(f\"'Functional' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#9通常売却と特異売却にわけた。\nabnormal_conditions = ['Abnorml', 'Family', 'Alloca', 'AdjLand','Partial']\n\n# 新しい列を作成（True＝特異売却）\nx_train['SaleCondition'] = x_train['SaleCondition'].isin(abnormal_conditions)\nx_test['SaleCondition'] = x_test['SaleCondition'].isin(abnormal_conditions)\n\n# 数値に変換（モデルに使う場合）#Abnormal で１　\nx_train['SaleCondition'] = x_train['SaleCondition'].astype(int)\nx_test['SaleCondition'] = x_test['SaleCondition'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#'AbnormalSaleのカラムを削除\n#x_train.drop(columns=['AbnormalSale'], inplace=True, errors='ignore')\n#x_test.drop(columns=['AbnormalSale'], inplace=True, errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### SaleConditionのマッピング後の確認","metadata":{}},{"cell_type":"code","source":"# #10\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_train['SaleCondition'].isna().sum()\n# print(f\"'SaleCondition' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #11\n# ###マッピング後のtest用データの型と値の種類を確認する\n# missing_count = x_test['SaleCondition'].isna().sum()\n# print(f\"'SaleCondition' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['SaleCondition'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# #マッピング後のテスト用データの平均、最大、最小を確認\n# x_test['SaleCondition'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#14\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['SaleCondition'].quantile(0.25)\n# q3 = x_train['SaleCondition'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['SaleCondition'] < lower) | (x_train['SaleCondition'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='SaleCondition', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"SaleCondition Outlier Visualization of SaleCondition\")\n# plt.ylabel('SaleCondition')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#15\n#マッピング後のtestデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['SaleCondition'].quantile(0.25)\n# q3 = x_test['SaleCondition'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['SaleCondition'] < lower) | (x_test['SaleCondition'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='SaleCondition', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"SaleCondition Outlier Visualization of SaleCondition\")\n# plt.ylabel('SaleCondition')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MSZoning\tのマッピング","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #テスト用データのカラム内のFunctionalの種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['MSZoning'].dtype)\n# print(x_train_backup['MSZoning'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###test用データの型と値の種類を確認する\n# print(x_test_backup['MSZoning'].dtype)\n# print(x_test_backup['MSZoning'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #test用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_train_backup['MSZoning'].isna().sum()\n# print(f\"'MSZoning' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###test用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_test_backup['MSZoning'].isna().sum()\n# print(f\"'MSZoning' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#9\n#'MSZoning マッピング　テストデータに4の欠損値であり、型はobject であることに留意\n#['RL' 'RM' 'C (all)' 'FV' 'RH'  学習用データ  \n#['RH' 'RL' 'RM' 'FV' 'C (all)' nan] testデータ\n#RL を高価値Residential RM,RH を低価値Residential,他を価値なしと３区分とした。大分類で２区分も可能とのこと。\n\n\nMSZoning_order = {\n    'RL': 2,\n    'RM': 1,\n    'RH': 1,\n    'C (all)': 0,\n    'FV': 0,  \n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['MSZoning'] = x_train['MSZoning'].map(MSZoning_order)\nx_test['MSZoning'] = x_test['MSZoning'].map(MSZoning_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MSZoningのマッピングの確認","metadata":{}},{"cell_type":"code","source":"# #10\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_train['MSZoning'].isna().sum()\n# print(f\"'MSZoning' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #11\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_test['MSZoning'].isna().sum()\n# print(f\"'MSZoning' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['MSZoning'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# #マッピング後のtest用データの平均、最大、最小を確認\n# x_test['MSZoning'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#14\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['MSZoning'].quantile(0.25)\n# q3 = x_train['MSZoning'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['MSZoning'] < lower) | (x_train['MSZoning'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='MSZoning', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"MSZoning Outlier Visualization of MSZoning\")\n# plt.ylabel('MSZoning')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#15\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['MSZoning'].quantile(0.25)\n# q3 = x_test['MSZoning'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['MSZoning'] < lower) | (x_test['MSZoning'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='MSZoning', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"MSZoning Outlier Visualization of MSZoning\")\n# plt.ylabel('MSZoning')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### MSSubClass のマッピング(欠損値なし　ともにint64)","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のMSSubClass の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #testデータのカラム内のMSSubClass の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['MSSubClass'].dtype)\n# print(x_train_backup['MSSubClass'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###test用データの型と値の種類を確認する\n# print(x_test_backup['MSSubClass'].dtype)\n# print(x_test_backup['MSSubClass'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #testデータでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_train_backup['MSSubClass'].isna().sum()\n# print(f\"'MSSubClass' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###test用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_test_backup['MSSubClass'].isna().sum()\n# print(f\"'MSSubClass' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #9\n# #学習用データの平均、最大、最小を確認\n# x_train_backup['MSSubClass'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #10\n# #test用データの平均、最大、最小を確認\n# x_test_backup['MSSubClass'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#11\n#マッピング（テスト、トレインともにint型をマッピングする）\n#int64\n#[ 60  20  70  50 190  45  90 120  30  85  80 160  75 180  40]学習\n#int64\n#[ 20  60 120 160  80  30  50  90  85 190  45  70  75 180  40 150]テスト\n\n\nMSSubClass_order = {\n     20: 3,\n     60: 3,\n     90: 3,\n     45: 2,\n     70: 2, \n     75: 2, \n     85: 2, \n     30: 1, \n     40: 1, \n     50: 1, \n     80: 1, \n    120: 1, \n    150: 1, \n    160: 0, \n    180: 0, \n    190: 0,  \n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['MSSubClass'] = x_train['MSSubClass'].map(MSSubClass_order)\nx_test['MSSubClass'] = x_test['MSSubClass'].map(MSSubClass_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# ###マッピング後の学習用データのMSSubClassの欠損件数を個別に確認する。\n# missing_count = x_train['MSSubClass'].isna().sum()\n# print(f\"'MSSubClass' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# ###マッピング後のtestデータのMSSubClassの欠損件数を個別に確認する。\n# missing_count = x_test['MSSubClass'].isna().sum()\n# print(f\"'MSSubClass' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #14\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['MSSubClass'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #15\n# #マッピング後のtest用データの平均、最大、最小を確認\n# x_test['MSSubClass'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#16\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['MSSubClass'].quantile(0.25)\n# q3 = x_train['MSSubClass'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['MSSubClass'] < lower) | (x_train['MSSubClass'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='MSSubClass', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"MSSubClass Outlier Visualization of MSSubClass\")\n# plt.ylabel('MSSubClass')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#16\n#マッピング後のtestデータの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['MSSubClass'].quantile(0.25)\n# q3 = x_test['MSSubClass'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['MSSubClass'] < lower) | (x_test['MSSubClass'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='MSSubClass', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"MSSubClass Outlier Visualization of MSSubClass\")\n# plt.ylabel('MSSubClass')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## SaleTypeのマッピング\t","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のSaleType の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #testデータのカラム内のMSSubClass の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['SaleType'].dtype)\n# print(x_train_backup['SaleType'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###学習用データの型と値の種類を確認する\n# print(x_test_backup['SaleType'].dtype)\n# print(x_test_backup['SaleType'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #test用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_train_backup['SaleType'].isna().sum()\n# print(f\"'SaleType' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###test用データのSaleConditionの欠損件数を個別に確認する。\n# missing_count = x_test_backup['SaleType'].isna().sum()\n# print(f\"'SaleType' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #9\n# # 平均・中央値・件数・標準偏差・最小・最大・外れ値 を集計するコード\n# import pandas as pd\n# import numpy as np\n\n# # IQRを使った外れ値カウント関数\n# def count_outliers(series):\n#     q1 = series.quantile(0.25)\n#     q3 = series.quantile(0.75)\n#     iqr = q3 - q1\n#     lower_bound = q1 - 1.5 * iqr\n#     upper_bound = q3 + 1.5 * iqr\n#     return ((series < lower_bound) | (series > upper_bound)).sum()\n\n# # SaleType ごとの SalePrice の統計量を集計\n# def summarize_sale_type_stats(df):\n#     stats = df.groupby(\"SaleType\")[\"SalePrice\"].agg(\n#         平均=\"mean\",\n#         中央値=\"median\",\n#         標準偏差=\"std\",\n#         件数=\"count\",\n#         最小=\"min\",\n#         最大=\"max\"\n#     )\n#     # 外れ値数を計算して追加\n#     stats[\"外れ値数\"] = df.groupby(\"SaleType\")[\"SalePrice\"].apply(count_outliers)\n#     return stats.sort_values(by=\"中央値\", ascending=True)  # 平均で昇順ソート\n\n# # 使用例（train_df を使って集計）\n# sale_type_stats = summarize_sale_type_stats(train_df)\n# print(sale_type_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #10\n# # 上記に関連して、sale_type_stats　で　SaleTypeごとのSalePriceをバイオリンプロットで可視化　\n# #  return stats.sort_values(by=\"中央値\", ascending=True)  # 平均で昇順ソート 　← (by=\"中央値\"の\" \"部分を任意で変更すればよい\n\n# plt.figure(figsize=(10, 6))\n# sns.violinplot(data=train_df, x=\"SaleType\", y=\"SalePrice\", order=sale_type_stats.index)\n# plt.title(\"SaleType　SalePrice　 (Vionlineplot)\")\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #11\n# # SaleType ごとの平均値を計算し、高い順に並べる\n# sale_type_mean = train_df.groupby(\"SaleType\")[\"SalePrice\"].mean().sort_values(ascending=False)\n\n# # 棒グラフ表示\n# plt.figure(figsize=(10, 6))\n# sale_type_mean.sort_values(ascending=True).plot(kind='barh', color='skyblue')  # 横棒グラフ\n# plt.xlabel('Mean SalePrice')\n# plt.title('SaleTypes mean Of SalePrice')\n# plt.tight_layout()\n# plt.grid(axis='x', linestyle='--', alpha=0.7)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# # SaleType ごとの中央値を計算し、高い順に並べる\n# sale_type_median = train_df.groupby(\"SaleType\")[\"SalePrice\"].median().sort_values(ascending=False)\n\n# # 棒グラフ表示\n# plt.figure(figsize=(10, 6))\n# sale_type_median.sort_values(ascending=True).plot(kind='barh', color='skyblue')  # 横棒グラフ\n# plt.xlabel('Median SalePrice')\n# plt.title('SaleTypes median Of SalePrice')\n# plt.tight_layout()\n# plt.grid(axis='x', linestyle='--', alpha=0.7)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#13\n#SaleType マッピング　テストデータに２の欠損値であり、型はobject であることに留意\n#WD, New, COD, ConLD, ConLI, CWD, ConLw, Con, Oth 左記は学習用データ\n#WD, COD, New, ConLD, Oth, Con, ConLw, ConLI, CWD  左記はテストデータ\n\n#5 ## Con      269600.0  New      247453.0\n#4 ## CWD      188750.0\n#3 ## WD       158000.0\n#2 ## ConLw    144000.0 ConLD    140000.0 COD      139000.0\n#1 ### ConLI    125000.0\n#0 ## Oth      116050.0        \n\n\n\n\nSaleType_order = {\n    'New': 5,\n    'Con': 5,\n    'CWD': 4,\n    'WD': 3,\n    'ConLw': 2,  \n    'ConLD': 2,\n    'COD': 2,\n    'ConLI': 1,\n    'Oth': 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['SaleType'] = x_train['SaleType'].map(SaleType_order)\nx_test['SaleType'] = x_test['SaleType'].map(SaleType_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### マッピング後の確認","metadata":{}},{"cell_type":"code","source":"# #10\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_train['SaleType'].isna().sum()\n# print(f\"'SaleType' のマッピング後の欠損値件数: {missing_count} 件\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #11\n# ###マッピング後のtest用データの型と値の種類を確認する\n# missing_count = x_test['SaleType'].isna().sum()\n# print(f\"'SaleType' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #12\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['SaleType'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_test['SaleType'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#14\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['SaleType'].quantile(0.25)\n# q3 = x_train['SaleType'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['SaleType'] < lower) | (x_train['SaleType'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='SaleType', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"SaleType Outlier Visualization of SaleType\")\n# plt.ylabel('SaleType')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#15\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['SaleType'].quantile(0.25)\n# q3 = x_test['SaleType'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['SaleType'] < lower) | (x_test['SaleType'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='SaleType', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"SaleType Outlier Visualization of SaleType\")\n# plt.ylabel('SaleType')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Neighborhood\tのマッピング","metadata":{}},{"cell_type":"code","source":"# #1\n# #学習用データのカラム内のSaleType の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_train_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #2\n# #学習用データのカラム内のSaleType の種類数とサンプル値を確認する。\n\n# summary = pd.DataFrame({\n#     '種類数': [x_test_backup[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test_backup[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n#     }, index=features)\n\n# #pandas のデフォルト表示幅制限を解除↓  \n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #3\n# ###学習用データの型と値の種類を確認する\n# print(x_train_backup['Neighborhood'].dtype)\n# print(x_train_backup['Neighborhood'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #4\n# ###test用データの型と値の種類を確認する\n# print(x_test_backup['Neighborhood'].dtype)\n# print(x_test_backup['Neighborhood'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #5\n# #学習用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_train_backup[features].isnull().sum(),\n#     '欠損率': x_train_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #6\n# #test用データでの欠損件数の確認\n\n# missing_info = pd.DataFrame({\n#     '欠損件数': x_test_backup[features].isnull().sum(),\n#     '欠損率': x_test_backup[features].isnull().mean()\n# })\n\n# # 欠損があるカラムだけ表示（任意）\n# missing_info = missing_info[missing_info['欠損件数'] > 0]\n# missing_info = missing_info.sort_values(by='欠損率', ascending=False)\n\n# missing_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #7\n# ###学習用データのNeighborhoodの欠損件数を個別に確認する。\n# missing_count = x_train_backup['Neighborhood'].isna().sum()\n# print(f\"'Neighborhood' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #8\n# ###test用データのNeighborhoodの欠損件数を個別に確認する。\n# missing_count = x_test_backup['Neighborhood'].isna().sum()\n# print(f\"'Neighborhood' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #9\n# # 平均・中央値・件数・標準偏差・最小・最大・外れ値 を集計するコード\n# import pandas as pd\n# import numpy as np\n\n# # IQRを使った外れ値カウント関数\n# def count_outliers(series):\n#     q1 = series.quantile(0.25)\n#     q3 = series.quantile(0.75)\n#     iqr = q3 - q1\n#     lower_bound = q1 - 1.5 * iqr\n#     upper_bound = q3 + 1.5 * iqr\n#     return ((series < lower_bound) | (series > upper_bound)).sum()\n\n# # SaleType ごとの SalePrice の統計量を集計\n# def summarize_Neighborhood_type_stats(df):\n#     stats = df.groupby(\"Neighborhood\")[\"SalePrice\"].agg(\n#         平均=\"mean\",\n#         中央値=\"median\",\n#         標準偏差=\"std\",\n#         件数=\"count\",\n#         最小=\"min\",\n#         最大=\"max\"\n#     )\n#     # 外れ値数を計算して追加\n#     stats[\"外れ値数\"] = df.groupby(\"Neighborhood\")[\"SalePrice\"].apply(count_outliers)\n#     return stats.sort_values(by=\"中央値\", ascending=False)  # 中央値で昇順ソート\n\n# # 使用例（train_df を使って集計）\n# Neighborhood_type_stats = summarize_Neighborhood_type_stats(train_df)\n# print(Neighborhood_type_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#10\n# 上記に関連して、Neighborhood_type_stats　で　NeighborhoodごとのSalePriceをバイオリンプロットで可視化　\n# return stats.sort_values(by=\"中央値\", ascending=False)  # 中央値で昇順ソート\n\n# plt.figure(figsize=(10, 6))\n# sns.violinplot(data=train_df, x=\"Neighborhood\", y=\"SalePrice\", order=Neighborhood_type_stats.index)\n# plt.title(\"Neighborhood　SalePrice　 (Vionlineplot)\")\n# plt.xticks(rotation=45)\n# plt.tight_layout()\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#11\n#Neighborhood マッピング　\n      \n#7 NridgHt       316270.623377  315000.0   96392.544954   77  154000  611657   \n#7 NoRidge       335295.317073  301500.0  121412.658640   41  190000  755000   \n#7 StoneBr       310499.000000  278000.0  112969.676640   25  170000  556581   \n\n#6 Timber        242247.447368  228475.0   64845.651549   38  137500  378500   \n#6 Somerst       225379.837209  225500.0   56177.555888   86  144152  423000   \n#6 Veenker       238772.727273  218000.0   72369.317959   11  162500  385000   \n#6 Crawfor       210624.725490  200624.0   68866.395472   51   90350  392500   \n#6 ClearCr       212565.428571  200250.0   50231.538993   28  130000  328000 \n\n#5 CollgCr       197965.773333  197200.0   51403.666438  150  110000  424870   \n#5 Blmngtn       194870.882353  191000.0   30393.229219   17  159895  264561   \n#5 NWAmes        189050.068493  182900.0   37172.218106   73   82500  299800   \n#5 Gilbert       192854.506329  181000.0   35986.779085   79  141000  377500   \n#5 SawyerW       186555.796610  179900.0   55651.997820   59   76000  320000\n\n\n#4 Mitchel       156270.122449  153500.0   36486.625334   49   84500  271000   \n#4 NPkVill       142694.444444  146000.0    9377.314529    9  127500  155000   \n#4 NAmes         145847.080000  140000.0   33075.345450  225   87500  345000  \n\n#3 SWISU         142591.360000  139500.0   32622.917679   25   60000  200000   \n#3 Blueste       137500.000000  137500.0   19091.883092    2  124000  151000   \n#3 Sawyer        136793.135135  135000.0   22345.129157   74   62383  190000 \n\n#2 BrkSide       124834.051724  124300.0   40348.689270   58   39300  223500   \n#2 Edwards       128219.700000  121750.0   43208.616459  100   58500  320000   \n#2 OldTown       128225.300885  119000.0   52650.583185  113   37900  475000 \n\n#1 BrDale        104493.750000  106000.0   14330.176493   16   83000  125000   \n#1 IDOTRR        100123.783784  103000.0   33376.710117   37   34900  169500   \n\n#0 MeadowV        98576.470588   88000.0   23491.049610   17   75000  151400 \n\n\nNeighborhood_order = {\n    'NridgHt': 7,\n    'NoRidge': 7,  \n    'StoneBr': 7,\n    'Timber': 6,\n    'Somerst': 6,\n    'Veenker': 6,\n    'Crawfor': 6,\n    'ClearCr': 6,\n    'CollgCr': 5,\n    'Blmngtn': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'SawyerW': 5,\n    'Mitchel': 4,\n    'NPkVill': 4,\n    'NAmes': 4,\n    'SWISU': 3,\n    'Blueste': 3,\n    'Sawyer': 3,\n    'BrkSide': 2,\n    'Edwards': 2,\n    'OldTown': 2,\n    'BrDale': 1,\n    'IDOTRR': 1,\n    'MeadowV': 0,\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['Neighborhood'] = x_train['Neighborhood'].map(Neighborhood_order)\nx_test['Neighborhood'] = x_test['Neighborhood'].map(Neighborhood_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### マッピング後の確認","metadata":{}},{"cell_type":"code","source":"# #12\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_train['Neighborhood'].isna().sum()\n# print(f\"'Neighborhood' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #13\n# ###マッピング後の学習用データの型と値の種類を確認する\n# missing_count = x_test['Neighborhood'].isna().sum()\n# print(f\"'Neighborhood' のマッピング後の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #14\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_train['Neighborhood'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #15\n# #マッピング後の学習用データの平均、最大、最小を確認\n# x_test['Neighborhood'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#16\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_train['Neighborhood'].quantile(0.25)\n# q3 = x_train['Neighborhood'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_train['Outlier_Flag'] = ((x_train['Neighborhood'] < lower) | (x_train['Neighborhood'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_train.index, y='Neighborhood', hue='Outlier_Flag', data=x_train, palette={False: 'blue', True: 'red'})\n# plt.title(\"Neighborhood Outlier Visualization of Neighborhood\")\n# plt.ylabel('Neighborhood')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#17\n#マッピング後の学習データの外れ値の確認\n# IQRを使った外れ値の範囲を計算\n\n\n# q1 = x_test['Neighborhood'].quantile(0.25)\n# q3 = x_test['Neighborhood'].quantile(0.75)\n# iqr = q3 - q1\n# lower = q1 - 1.5 * iqr\n# upper = q3 + 1.5 * iqr\n\n# # 外れ値のフラグを追加（True = 外れ値）\n# x_test['Outlier_Flag'] = ((x_test['Neighborhood'] < lower) | (x_test['Neighborhood'] > upper))\n\n# # 散布図で可視化（外れ値だけ赤く）\n# plt.figure(figsize=(10, 5))\n# sns.scatterplot(x=x_test.index, y='Neighborhood', hue='Outlier_Flag', data=x_test, palette={False: 'blue', True: 'red'})\n# plt.title(\"Neighborhood Outlier Visualization of Neighborhood\")\n# plt.ylabel('Neighborhood')\n# plt.xlabel('date index')\n# plt.legend(title='Outlier?', labels=['Normal', 'Outlier'])\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## OverallQualの処理など（相関関係大、外れ値あり、float、歪度小、欠損値なし、マッピング処理なし）","metadata":{}},{"cell_type":"markdown","source":"### 1　目的変数との相関関係の確認","metadata":{}},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます。\n# correlation = df_1['OverallQual'].corr(df_1['SalePrice'])\n\n# print(\"CSV→データフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ２ 目的変数との相関関係の確認と説明","metadata":{}},{"cell_type":"code","source":"# # データフレームから直接計算\n# correlation = df_1['OverallQual'].corr(df_1['SalePrice'])\n\n\n# # --- 結果の表示 ---\n# print(\"--- OverallQual と SalePrice の相関係数 ---\")\n# print(f\"{correlation:.4f}\") # 小数点以下4桁で表示\n\n# print(\"\\n【解説】\")\n# if correlation > 0.7:\n#     print(\"-> 非常に強い正の相関があります。\")\n#     print(\"   これは、OverallQual（総合的な品質）が高くなるほど、\")\n#     print(\"   SalePrice（住宅価格）も顕著に高くなる傾向があることを示しています。\")\n# elif correlation > 0.4:\n#     print(\"-> やや強い正の相関があります。\")\n# else:\n#     print(\"-> 正の相関があります。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 3 目的変数に対しての外れ値の確認","metadata":{}},{"cell_type":"code","source":"# --- グラフ描画のためのデータ準備 ---\n# x_train と y_train をグラフ描画のために一時的に結合します。\n# これにより、'OverallQual' と 'SalePrice' を同時に扱えるようになります。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# --- グラフの描画 (箱ひげ図) ---\n# print(\"\\n--- グラフを生成します ---\")\n# plt.figure(figsize=(12, 8)) # グラフのサイズを大きめに設定\n\n# sns.boxplot(x='OverallQual', y='SalePrice', data=plotting_data)\n\n# # グラフのタイトルとラベルを設定\n# plt.title('OverallQual_with_SalePrice_plost（Outs）', fontsize=16)\n# plt.xlabel('OverallQual (Toatal Quall)', fontsize=12)\n# plt.ylabel('SalePrice', fontsize=12)\n\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n# plt.show()\n\n\n# # --- 補足：散布図による確認 ---\n# plt.figure(figsize=(12, 8))\n# sns.scatterplot(x='OverallQual', y='SalePrice', data=plotting_data, alpha=0.6)\n# plt.title('OverallQual with SalePrice plots', fontsize=16)\n# plt.xlabel('OverallQual (Toatal Quall)', fontsize=12)\n# plt.ylabel('SalePrice (SalePrice)', fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.6)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 4　目的変数に対しての外れ値の数値詳細","metadata":{}},{"cell_type":"code","source":"# --- 1. グラフと計算のためのデータ準備 ---\n# x_train と y_train を一時的に結合します。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# --- 2. 外れ値の計算 ---\n# OverallQual の各カテゴリごとに、Q1, Q3, IQR を計算\n# Q1 = plotting_data.groupby('OverallQual')['SalePrice'].quantile(0.25)\n# Q3 = plotting_data.groupby('OverallQual')['SalePrice'].quantile(0.75)\n# IQR = Q3 - Q1\n\n# # 外れ値の境界線（下限と上限）を計算\n# lower_bound = Q1 - 1.5 * IQR\n# upper_bound = Q3 + 1.5 * IQR\n\n# # 計算結果を一つのデータフレームにまとめる\n# outlier_stats = pd.DataFrame({\n#     'Q1': Q1,\n#     'Q3': Q3,\n#     'IQR': IQR,\n#     'lower_bound': lower_bound,\n#     'upper_bound': upper_bound\n# })\n# print(\"\\n--- OverallQualごとの外れ値の境界線 ---\")\n# print(outlier_stats)\n\n\n# # --- 3. 外れ値の特定と件数の表示 ---\n# # 各行のデータが、そのOverallQualカテゴリの境界線の外にあるかどうかを判定\n# # .any() は、1行でもTrueがあればTrueを返すので、ここでは使いません。\n# # 各行ごとに判定するため、ここではループやapplyを使います。\n\n# outliers_list = []\n# for index, row in plotting_data.iterrows():\n#     qual = row['OverallQual']\n#     price = row['SalePrice']\n    \n#     # その行の品質レベルに対応する境界線を取得\n#     bounds = outlier_stats.loc[qual]\n    \n#     # 境界線の外にあるかどうかをチェック\n#     if price < bounds['lower_bound'] or price > bounds['upper_bound']:\n#         outliers_list.append(row)\n\n# # 外れ値のリストを新しいデータフレームに変換\n# outliers_df = pd.DataFrame(outliers_list)\n\n\n# print(f\"\\n--- 外れ値の件数 ---\")\n# print(f\"合計 {len(outliers_df)} 件の外れ値が検出されました。\")\n\n# # OverallQualごとに何件の外れ値があったかを表示\n# print(\"\\n--- 品質レベルごとの外れ値の件数 ---\")\n# print(outliers_df['OverallQual'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0-1正規分布に近いかどうか確認する。\n# ただし、このような特徴量は、１～１０と明確な順序があるため、対数変換は有用な「順序情報」を歪めてしまうため、むしろ精度を悪化させる。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#0-2実践的な判断フロー\n#計算: 特徴量の歪度を計算する (.skew())。\n#閾値チェック: 絶対値が 0.5 や 1.0 を超えているか確認する。\n#性質チェック: それは変換可能な「連続値」か？ (OverallQual のようなものではないか？)\n#可視化: ヒストグラムで分布の形を目で見て確認する。\n#実験: 変換した場合としない場合で、モデルの精度を比較してみる。\n#この多角的な視点を持つことが、より精度の高いモデルを作成するための鍵となります。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 5 データ（選択した特徴量）の形の確認","metadata":{}},{"cell_type":"code","source":"#学習用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_train['OverallQual'], kde=True)\n# plt.title('Before OverallQual(train) plots')\n# plt.xlabel('OverallQual')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#テスト用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_test['OverallQual'], kde=True)\n# plt.title('Before OverallQual(test) plots')\n# plt.xlabel('OverallQual')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 6 データ（選択した特徴量）の歪度の確認","metadata":{}},{"cell_type":"code","source":"#testデータのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_test['OverallQual'], kde=True)\n# plt.title('Before OverallQual(test) plots')\n# plt.xlabel('OverallQual')\n# plt.ylabel('度数')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### ７ データ（選択した特徴量）の歪度への説明","metadata":{}},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'OverallQual' の歪度を計算 ---\n# skewness = x_train['OverallQual'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" OverallQualの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #test用データの歪度を調べる\n# # --- 1. 'OverallQual' の歪度を計算 ---\n# skewness = x_test['OverallQual'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" OverallQualの歪度（テスト用）: {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布です。対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8 データ（選択した特徴量）の型と種類の確認","metadata":{}},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['OverallQual'].dtype)\n# print(x_train['OverallQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###test用データの型と値の種類を確認する\n# print(x_test['OverallQual'].dtype)\n# print(x_test['OverallQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9 データ（選択した特徴量）の統計情報とそのグラフの確認","metadata":{}},{"cell_type":"code","source":"# qual_series = x_train['OverallQual']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('OverallQual of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('OverallQual')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('OverallQual')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_test['OverallQual']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('OverallQual of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('OverallQual')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('OverallQual')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 10 データ（選択した特徴量）の欠損値の確認","metadata":{}},{"cell_type":"code","source":"# ###学習用データのOverallQualの欠損件数を個別に確認する。\n# missing_count = x_train['OverallQual'].isna().sum()\n# print(f\"'OverallQual(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###test用データのOverallQualの欠損件数を個別に確認する。\n# missing_count = x_test['OverallQual'].isna().sum()\n# print(f\"'OverallQual（テスト用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FullBath\t特徴量の追加と確認","metadata":{}},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます。\n# correlation = df_1['FullBath'].corr(df_1['SalePrice'])\n\n# print(\"CSV→データフレームで直接計算した相関係数:\")\n# print(correlation)\n\n# print(\"\\n【解説】\")\n# if correlation > 0.7:\n#     print(\"-> 非常に強い正の相関があります。\")\n#     print(\"   これは、FullBath（バスルームの数）が多くなるほど、\")\n#     print(\"   SalePrice（住宅価格）も顕著に高くなる傾向があることを示しています。\")\n# elif correlation > 0.4:\n#     print(\"-> やや強い正の相関があります。\")\n# else:\n#     print(\"-> 正の相関があります。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- グラフ描画のためのデータ準備 ---\n# # x_train と y_train をグラフ描画のために一時的に結合します。\n# # これにより、'OverallQual' と 'SalePrice' を同時に扱えるようになります。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- グラフの描画 (箱ひげ図) ---\n# print(\"\\n--- グラフを生成します ---\")\n# plt.figure(figsize=(12, 8)) # グラフのサイズを大きめに設定\n\n# sns.boxplot(x='FullBath', y='SalePrice', data=plotting_data)\n\n# # グラフのタイトルとラベルを設定\n# plt.title('FullBath_with_SalePrice_plost（Outs）', fontsize=16)\n# plt.xlabel('FullBath (Toatal BathRooms)', fontsize=12)\n# plt.ylabel('SalePrice', fontsize=12)\n\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n# plt.show()\n\n\n# # --- 補足：散布図による確認 ---\n# plt.figure(figsize=(12, 8))\n# sns.scatterplot(x='FullBath', y='SalePrice', data=plotting_data, alpha=0.6)\n# plt.title('FullBath with SalePrice plots', fontsize=16)\n# plt.xlabel('FullBath (Toatal BathRooms)', fontsize=12)\n# plt.ylabel('SalePrice (SalePrice)', fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.6)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 1. グラフと計算のためのデータ準備 ---\n# # x_train と y_train を一時的に結合します。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- 2. 外れ値の計算 ---\n# # OverallQual の各カテゴリごとに、Q1, Q3, IQR を計算\n# Q1 = plotting_data.groupby('FullBath')['SalePrice'].quantile(0.25)\n# Q3 = plotting_data.groupby('FullBath')['SalePrice'].quantile(0.75)\n# IQR = Q3 - Q1\n\n# # 外れ値の境界線（下限と上限）を計算\n# lower_bound = Q1 - 1.5 * IQR\n# upper_bound = Q3 + 1.5 * IQR\n\n# # 計算結果を一つのデータフレームにまとめる\n# outlier_stats = pd.DataFrame({\n#     'Q1': Q1,\n#     'Q3': Q3,\n#     'IQR': IQR,\n#     'lower_bound': lower_bound,\n#     'upper_bound': upper_bound\n# })\n# print(\"\\n--- FullBathごとの外れ値の境界線 ---\")\n# print(outlier_stats)\n\n\n# # --- 3. 外れ値の特定と件数の表示 ---\n# # 各行のデータが、そのOverallQualカテゴリの境界線の外にあるかどうかを判定\n# # .any() は、1行でもTrueがあればTrueを返すので、ここでは使いません。\n# # 各行ごとに判定するため、ここではループやapplyを使います。\n\n# outliers_list = []\n# for index, row in plotting_data.iterrows():\n#     qual = row['FullBath']\n#     price = row['SalePrice']\n    \n#     # その行の品質レベルに対応する境界線を取得\n#     bounds = outlier_stats.loc[qual]\n    \n#     # 境界線の外にあるかどうかをチェック\n#     if price < bounds['lower_bound'] or price > bounds['upper_bound']:\n#         outliers_list.append(row)\n\n# # 外れ値のリストを新しいデータフレームに変換\n# outliers_df = pd.DataFrame(outliers_list)\n\n\n# print(f\"\\n--- 外れ値の件数 ---\")\n# print(f\"合計 {len(outliers_df)} 件の外れ値が検出されました。\")\n\n# # OverallQualごとに何件の外れ値があったかを表示\n# print(\"\\n--- 品質レベルごとの外れ値の件数 ---\")\n# print(outliers_df['FullBath'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_train['FullBath'], kde=True)\n# plt.title('Before FullBath(train) plots')\n# plt.xlabel('FullBath')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_test['FullBath'], kde=True)\n# plt.title('Before FullBath(train) plots')\n# plt.xlabel('FullBath')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['FullBath'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" FullBathの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # test用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_test['FullBath'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" FullBathの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データの型と値の種類を確認する\n# print(x_train['FullBath'].dtype)\n# print(x_train['FullBath'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###test用データの型と値の種類を確認する\n# ###testデータに４という学習データにない値があるので、これを学習させる必要がある。\n# print(x_test['FullBath'].dtype)\n# print(x_test['FullBath'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_train['FullBath']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('FullBath of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('FullBath')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('FullBath')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_test['FullBath']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('FullBath of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('OverallQual')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('FullBath')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###学習用データのFullBathの欠損件数を個別に確認する。\n# missing_count = x_train['FullBath'].isna().sum()\n# print(f\"'FullBath(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###test用データのFullBathの欠損件数を個別に確認する。\n# missing_count = x_test['FullBath'].isna().sum()\n# print(f\"'FullBath(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#FullBath の特徴量は、学習用データでは１，２，３、０　の４種類　テストデータでは１，２，３，４，０ の５種類を考慮したマッピング\nFullBath_order = {\n     4: '4',\n     3: '3',\n     2: '2',  \n     1: '1',\n     0: '0',\n}\n\n\n# 数値へマッピング（安全で一貫性のある変換）\nx_train['FullBath'] = x_train['FullBath'].map(FullBath_order)\nx_test['FullBath'] = x_test['FullBath'].map(FullBath_order)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Objectで出力される型をintにキャストしておく\nx_train['FullBath'] = x_train['FullBath'].astype(int)\nx_test['FullBath'] = x_test['FullBath'].astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###マッピング後の学習用データの型と値の種類を確認する\n# print(x_train['FullBath'].dtype)\n# print(x_train['FullBath'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###マッピング後のtest用データの型と値の種類を確認する\n# print(x_test['FullBath'].dtype)\n# print(x_test['FullBath'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 2. 'FullBath' が 4 の物件を絞り込む ---\n# # df_train['FullBath'] == 4 という条件に合う行だけを抽出します。\n# four_bath_houses = x_test[x_test['FullBath'] == 4].copy()\n\n\n# # --- 3. 該当する物件の情報を表示 ---\n# if not four_bath_houses.empty:\n#     print(f\"--- testデータ内で 'FullBath' が 4 の物件は {len(four_bath_houses)} 件見つかりました ---\")\n    \n    \n# else:\n#     print(\"学習データ内には 'FullBath' が 4 の物件は見つかりませんでした。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 2. 'FullBath' が 4 の物件を絞り込む ---\n# # df_train['FullBath'] == 4 という条件に合う行だけを抽出します。\n# four_bath_houses = x_train[x_train['FullBath'] == 4].copy()\n\n\n# # --- 3. 該当する物件の情報を表示 ---\n# if not four_bath_houses.empty:\n#     print(f\"--- 学習データ内で 'FullBath' が 4 の物件は {len(four_bath_houses)} 件見つかりました ---\")\n    \n    \n# else:\n#     print(\"学習データ内には 'FullBath' が 4 の物件は見つかりませんでした。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 特徴量新規追加の例　YearBuilt と　YrSold（x_train['YrSold'] - x_train['YearBuilt']）＝x_train Age の特徴量の追加","metadata":{}},{"cell_type":"code","source":"# ###学習用データ（futures に入れる前）の型と値の種類を確認する\n# print(train_df['YearBuilt'].dtype)\n# print(train_df['YearBuilt'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ####学習用データのYearBuiltの欠損件数を個別に確認する。\n# missing_count = train_df['YearBuilt'].isna().sum()\n# print(f\"'YearBuilt(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ###test用データ（futures に入れる前）の型と値の種類を確認する\n# print(test['YearBuilt'].dtype)\n# print(test['YearBuilt'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ####test用データのYearBuiltの欠損件数を個別に確認する。\n# missing_count = test['YearBuilt'].isna().sum()\n# print(f\"'YearBuilt(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##学習用データ（futures に入れる前）の型と値の種類を確認する\n# print(train_df['YrSold'].dtype)\n# print(train_df['YrSold'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ####学習用データのYrSoldの欠損件数を個別に確認する。\n# missing_count = train_df['YrSold'].isna().sum()\n# print(f\"'YrSold(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##test用データ（futures に入れる前）の型と値の種類を確認する\n# print(test['YrSold'].dtype)\n# print(test['YrSold'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ####test用データのYrSoldの欠損件数を個別に確認する。\n# missing_count = test['YrSold'].isna().sum()\n# print(f\"'YrSold(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#YearBuilt と　YrSold は共にint64であり、欠損値なし。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- ここが追加するコード ---\n# 'Age'という新しい列に、'YrSold'列と'YearBuilt'列の差を代入する.負の相関関係となる。\n#また、x_train はfutures で選択後のデータであり、この場合選択されていないためエラーとなる。よって上記引き算は,特徴量選択前のtrain_df で行えばよい。\nx_train['Age'] = train_df['YrSold'] - train_df['YearBuilt']\nx_test['Age'] = test['YrSold'] - test['YearBuilt']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#新しい特徴量がx_train(特徴量をfuturesでいれたデータ)に反映されていることを末尾で確認\n# x_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#新しい特徴量がx_test(特徴量をfuturesでいれたデータ)に反映されていることを末尾で確認\n# x_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['Age'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#学習用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_train['Age'], kde=True)\n# plt.title('Before Age(train) plots')\n# plt.xlabel('Age')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#test用データのヒストグラムの表示\n# plt.figure(figsize=(12, 5))\n\n# # 左側のプロット：変換前のヒストグラム\n# plt.subplot(1, 2, 1)\n# sns.histplot(x_test['Age'], kde=True)\n# plt.title('Before Age(test) plots')\n# plt.xlabel('Age')\n# plt.ylabel('Counts')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##新規作成後の学習用データ型と値の種類を確認する\n# print(x_train['Age'].dtype)\n# print(x_train['Age'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##新規作成後のテスト用データ型と値の種類を確認する\n# print(x_test['Age'].dtype)\n# print(x_test['Age'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- グラフ描画のためのデータ準備 ---\n# # x_train と y_train をグラフ描画のために一時的に結合します。\n# # これにより、'OverallQual' と 'SalePrice' を同時に扱えるようになります。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- グラフの描画 (箱ひげ図) ---\n# print(\"\\n--- グラフを生成します ---\")\n# plt.figure(figsize=(12, 8)) # グラフのサイズを大きめに設定\n\n# sns.boxplot(x='Age', y='SalePrice', data=plotting_data)\n\n# # グラフのタイトルとラベルを設定\n# plt.title('Age_with_SalePrice_plost（Outs）', fontsize=16)\n# plt.xlabel('Age (New Old)', fontsize=12)\n# plt.ylabel('SalePrice', fontsize=12)\n\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n# plt.show()\n\n\n# # --- 補足：散布図による確認 ---\n# plt.figure(figsize=(12, 8))\n# sns.scatterplot(x='Age', y='SalePrice', data=plotting_data, alpha=0.6)\n# plt.title('Age with SalePrice plots', fontsize=16)\n# plt.xlabel('Age (New Old)', fontsize=12)\n# plt.ylabel('SalePrice (SalePrice)', fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.6)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 1. グラフと計算のためのデータ準備 ---\n# # x_train と y_train を一時的に結合します。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- 2. 外れ値の計算 ---\n# # OverallQual の各カテゴリごとに、Q1, Q3, IQR を計算\n# Q1 = plotting_data.groupby('Age')['SalePrice'].quantile(0.25)\n# Q3 = plotting_data.groupby('Age')['SalePrice'].quantile(0.75)\n# IQR = Q3 - Q1\n\n# # 外れ値の境界線（下限と上限）を計算\n# lower_bound = Q1 - 1.5 * IQR\n# upper_bound = Q3 + 1.5 * IQR\n\n# # 計算結果を一つのデータフレームにまとめる\n# outlier_stats = pd.DataFrame({\n#     'Q1': Q1,\n#     'Q3': Q3,\n#     'IQR': IQR,\n#     'lower_bound': lower_bound,\n#     'upper_bound': upper_bound\n# })\n# print(\"\\n--- Ageごとの外れ値の境界線 ---\")\n# print(outlier_stats)\n\n\n# # --- 3. 外れ値の特定と件数の表示 ---\n# # 各行のデータが、そのOverallQualカテゴリの境界線の外にあるかどうかを判定\n# # .any() は、1行でもTrueがあればTrueを返すので、ここでは使いません。\n# # 各行ごとに判定するため、ここではループやapplyを使います。\n\n# outliers_list = []\n# for index, row in plotting_data.iterrows():\n#     qual = row['Age']\n#     price = row['SalePrice']\n    \n#     # その行の品質レベルに対応する境界線を取得\n#     bounds = outlier_stats.loc[qual]\n    \n#     # 境界線の外にあるかどうかをチェック\n#     if price < bounds['lower_bound'] or price > bounds['upper_bound']:\n#         outliers_list.append(row)\n\n# # 外れ値のリストを新しいデータフレームに変換\n# outliers_df = pd.DataFrame(outliers_list)\n\n\n# print(f\"\\n--- 外れ値の件数 ---\")\n# print(f\"合計 {len(outliers_df)} 件の外れ値が検出されました。\")\n\n# # OverallQualごとに何件の外れ値があったかを表示\n# print(\"\\n--- 品質レベルごとの外れ値の件数 ---\")\n# print(outliers_df['Age'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_train['Age']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('Age of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('Age')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('Age')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 特徴量リフォームからの年数追加","metadata":{}},{"cell_type":"code","source":"# print(train_df['YearRemodAdd'].dtype)\n# print(train_df['YearRemodAdd'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(test['YearRemodAdd'].dtype)\n# print(test['YearRemodAdd'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = train_df['YearRemodAdd'].isna().sum()\n# print(f\"'YearBuilt(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = test['YearRemodAdd'].isna().sum()\n# print(f\"'YearBuilt(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#また、x_train はfutures で選択後のデータであり、この場合選択されていないためエラーとなる。よって上記引き算は,特徴量選択前のtrain_df で行えばよい。\nx_train['YearsSinceRemodel'] = train_df['YrSold'] - train_df['YearRemodAdd']\nx_test['YearsSinceRemodel'] = test['YrSold'] - test['YearRemodAdd']\n# 'YearsSinceRemodel' 特徴量を作成","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['YearsSinceRemodel'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- グラフ描画のためのデータ準備 ---\n# # x_train と y_train をグラフ描画のために一時的に結合します。\n# # これにより、'YearsSinceRemodel' と 'SalePrice' を同時に扱えるようになります。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- グラフの描画 (箱ひげ図) ---\n# print(\"\\n--- グラフを生成します ---\")\n# plt.figure(figsize=(12, 8)) # グラフのサイズを大きめに設定\n\n# sns.boxplot(x='Age', y='SalePrice', data=plotting_data)\n\n# # グラフのタイトルとラベルを設定\n# plt.title('YearsSinceRemodel_with_SalePrice_plost（Outs）', fontsize=16)\n# plt.xlabel('YearsSinceRemodel (New Old)', fontsize=12)\n# plt.ylabel('YearsSinceRemodel', fontsize=12)\n\n# plt.grid(axis='y', linestyle='--', alpha=0.7)\n# plt.show()\n\n\n# # --- 補足：散布図による確認 ---\n# plt.figure(figsize=(12, 8))\n# sns.scatterplot(x='YearsSinceRemodel', y='SalePrice', data=plotting_data, alpha=0.6)\n# plt.title('YearsSinceRemodel with SalePrice plots', fontsize=16)\n# plt.xlabel('YearsSinceRemodel (New Old)', fontsize=12)\n# plt.ylabel('SalePrice (SalePrice)', fontsize=12)\n# plt.grid(True, linestyle='--', alpha=0.6)\n# plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_train['YearsSinceRemodel']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('YearsSinceRemodel of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('YearsSinceRemodel')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('YearsSinceRemodel')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 1. グラフと計算のためのデータ準備 ---\n# # x_train と y_train を一時的に結合します。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- 2. 外れ値の計算 ---\n# # OverallQual の各カテゴリごとに、Q1, Q3, IQR を計算\n# Q1 = plotting_data.groupby('YearsSinceRemodel')['SalePrice'].quantile(0.25)\n# Q3 = plotting_data.groupby('YearsSinceRemodel')['SalePrice'].quantile(0.75)\n# IQR = Q3 - Q1\n\n# # 外れ値の境界線（下限と上限）を計算\n# lower_bound = Q1 - 1.5 * IQR\n# upper_bound = Q3 + 1.5 * IQR\n\n# # 計算結果を一つのデータフレームにまとめる\n# outlier_stats = pd.DataFrame({\n#     'Q1': Q1,\n#     'Q3': Q3,\n#     'IQR': IQR,\n#     'lower_bound': lower_bound,\n#     'upper_bound': upper_bound\n# })\n# print(\"\\n--- YearsSinceRemodelごとの外れ値の境界線 ---\")\n# print(outlier_stats)\n\n\n# # --- 3. 外れ値の特定と件数の表示 ---\n# # 各行のデータが、そのOverallQualカテゴリの境界線の外にあるかどうかを判定\n# # .any() は、1行でもTrueがあればTrueを返すので、ここでは使いません。\n# # 各行ごとに判定するため、ここではループやapplyを使います。\n\n# outliers_list = []\n# for index, row in plotting_data.iterrows():\n#     qual = row['YearsSinceRemodel']\n#     price = row['SalePrice']\n    \n#     # その行の品質レベルに対応する境界線を取得\n#     bounds = outlier_stats.loc[qual]\n    \n#     # 境界線の外にあるかどうかをチェック\n#     if price < bounds['lower_bound'] or price > bounds['upper_bound']:\n#         outliers_list.append(row)\n\n# # 外れ値のリストを新しいデータフレームに変換\n# outliers_df = pd.DataFrame(outliers_list)\n\n\n# print(f\"\\n--- 外れ値の件数 ---\")\n# print(f\"合計 {len(outliers_df)} 件の外れ値が検出されました。\")\n\n# # YearsSinceRemodelごとに何件の外れ値があったかを表示\n# print(\"\\n--- YearsSinceRemodelごとの外れ値の件数 ---\")\n# print(outliers_df['YearsSinceRemodel'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 新築フラグの追加(IsNew はOverallQual で説明済みであり、精度が全く変わらなかったので、削除)","metadata":{}},{"cell_type":"code","source":"\n#x_train['IsNew'] = (train_df['YearBuilt'] == train_df['YrSold']).astype(int)\n#x_test['IsNew'] = (test['YearBuilt'] == test['YrSold']).astype(int)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_trainに'OverallQual'があると仮定\n#print(x_train.groupby('IsNew')['OverallQual'].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- IsNew はOverallQual で説明済みであり、精度が全く変わらなかったので、削除した---\n# x_train から 'IsNew' 列を削除\n#x_train.drop('IsNew', axis=1, inplace=True, errors='ignore')\n\n# x_test から 'IsNew' 列を削除\n#x_test.drop('IsNew', axis=1, inplace=True, errors='ignore')\n# -------","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TotalBsmtSF2の作成 = df['1stFlrSF'] + df['2ndFlrSF']）","metadata":{}},{"cell_type":"code","source":"x_train['TotalBsmtSF2'] = train_df2['1stFlrSF'] + test2['2ndFlrSF']\nx_test['TotalBsmtSF2'] = test2['1stFlrSF'] + test2['2ndFlrSF']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['TotalBsmtSF2'] = scaler.fit_transform(x_train[['TotalBsmtSF2']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['TotalBsmtSF2'] = scaler.transform(x_test[['TotalBsmtSF2']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # print(x_train['TotalBsmtSF2'].dtype)\n # print(x_train['TotalBsmtSF2'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#データフレームから直接計算\n# データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['TotalBsmtSF2'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_train['TotalBsmtSF2']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('TotalBsmtSF2 of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('TotalBsmtSF')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('TotalBsmtSF2')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # --- 1. グラフと計算のためのデータ準備 ---\n# # x_train と y_train を一時的に結合します。\n# plotting_data = pd.concat([x_train, y_train], axis=1)\n\n\n# # --- 2. 外れ値の計算 ---\n# # OverallQual の各カテゴリごとに、Q1, Q3, IQR を計算\n# Q1 = plotting_data.groupby('TotalBsmtSF')['SalePrice'].quantile(0.25)\n# Q3 = plotting_data.groupby('TotalBsmtSF')['SalePrice'].quantile(0.75)\n# IQR = Q3 - Q1\n\n# # 外れ値の境界線（下限と上限）を計算\n# lower_bound = Q1 - 1.5 * IQR\n# upper_bound = Q3 + 1.5 * IQR\n\n# # 計算結果を一つのデータフレームにまとめる\n# outlier_stats = pd.DataFrame({\n#     'Q1': Q1,\n#     'Q3': Q3,\n#     'IQR': IQR,\n#     'lower_bound': lower_bound,\n#     'upper_bound': upper_bound\n# })\n# print(\"\\n--- TotalBsmtSFごとの外れ値の境界線 ---\")\n# print(outlier_stats)\n\n\n# # --- 3. 外れ値の特定と件数の表示 ---\n# # 各行のデータが、そのOverallQualカテゴリの境界線の外にあるかどうかを判定\n# # .any() は、1行でもTrueがあればTrueを返すので、ここでは使いません。\n# # 各行ごとに判定するため、ここではループやapplyを使います。\n\n# outliers_list = []\n# for index, row in plotting_data.iterrows():\n#     qual = row['TotalBsmtSF']\n#     price = row['SalePrice']\n    \n#     # その行の品質レベルに対応する境界線を取得\n#     bounds = outlier_stats.loc[qual]\n    \n#     # 境界線の外にあるかどうかをチェック\n#     if price < bounds['lower_bound'] or price > bounds['upper_bound']:\n#         outliers_list.append(row)\n\n# # 外れ値のリストを新しいデータフレームに変換\n# outliers_df = pd.DataFrame(outliers_list)\n\n\n# print(f\"\\n--- 外れ値の件数 ---\")\n# print(f\"合計 {len(outliers_df)} 件の外れ値が検出されました。\")\n\n# # YearsSinceRemodelごとに何件の外れ値があったかを表示\n# print(\"\\n--- TotalBsmtSFごとの外れ値の件数 ---\")\n# print(outliers_df['TotalBsmtSF'].value_counts().sort_index())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['TotalBsmtSF2'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" TotalBsmtSFの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TotalSF （TotalBsmtSF + 1stFlrSF + 2ndFlrSF）の作成","metadata":{}},{"cell_type":"code","source":"x_train['TotalBsmtSF3'] = train_df2['1stFlrSF'] + test2['2ndFlrSF']\nx_test['TotalBsmtSF3'] = test2['1stFlrSF'] + test2['2ndFlrSF']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train['TotalSF'] = train_df2['1stFlrSF'] + test2['2ndFlrSF'] + x_train['TotalBsmtSF3']\nx_test['TotalSF'] = test2['1stFlrSF'] + test2['2ndFlrSF'] + x_test['TotalBsmtSF3']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train['TotalSF2'] = train_df2['1stFlrSF'] + test2['2ndFlrSF'] + x_train['TotalBsmtSF3']\n# x_test['TotalSF2'] = test2['1stFlrSF'] + test2['2ndFlrSF'] + x_test['TotalBsmtSF3']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train.drop('TotalBsmtSF2', axis=1, inplace=True)\n# x_test.drop('TotalBsmtSF2', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train.drop('TotalBsmtSF3', axis=1, inplace=True)\n# x_test.drop('TotalBsmtSF3', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['TotalSF'] = scaler.fit_transform(x_train[['TotalSF']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['TotalSF'] = scaler.transform(x_test[['TotalSF']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 'OverallGrade'] = df['OverallQual'] * df['OverallCond'] の作成","metadata":{}},{"cell_type":"code","source":"# print(train_df['OverallQual'].dtype)\n# print(train_df['OverallQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(test['OverallQual'].dtype)\n# print(test['OverallQual'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = train_df['OverallQual'].isna().sum()\n# print(f\"'OverallQual(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = test['OverallQual'].isna().sum()\n# print(f\"'OverallQual(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(train_df['OverallCond'].dtype)\n# print(train_df['OverallCond'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(test['OverallCond'].dtype)\n# print(test['OverallCond'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = train_df['OverallCond'].isna().sum()\n# print(f\"'OverallCond(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = test['OverallCond'].isna().sum()\n# print(f\"'OverallCond(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train['OverallGrade'] = train_df['OverallQual'] * train_df['OverallCond']\nx_test['OverallGrade'] = test['OverallQual'] * test['OverallCond']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['OverallGrade'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_train['OverallGrade']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('OverallGrade of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('OverallGrade')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('OverallGrade')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['OverallGrade'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" OverallGradeの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(x_test['OverallGrade'].dtype)\n# print(x_test['OverallGrade'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(x_train['OverallGrade'].dtype)\n# print(x_train['OverallGrade'].unique())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_train['OverallGrade'].isna().sum()\n# print(f\"'OverallGrade(学習用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# missing_count = x_test['OverallGrade'].isna().sum()\n# print(f\"'OverallGrade(test用）' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TotalBath の追加　とまとめてカラム表示、欠損値を最頻値で埋めた例","metadata":{}},{"cell_type":"code","source":"# cols_to_check = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = train_df[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = train_df[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = train_df[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 学習データ(x_train)の最頻値を計算する\n# .mode()は最頻値をSeriesとして返すため、[0]で最初の値(最も頻度が高い値)を取得\nmode_value = test['BsmtFullBath'].mode()[0]\n\n# 2. 計算した最頻値で、テストデータ(x_test)の欠損値を埋める\n# .fillna() を使います。inplace=Trueで元のDataFrameを直接変更します。\ntest['BsmtFullBath'].fillna(mode_value, inplace=True)\n\nprint(f\"\\testデータの'BsmtFullBath'の最頻値は: {mode_value}\")\n\n# print(\"\\n--- 処理後の test ---\")\n# print(test['BsmtFullBath'])\n\nprint(\"\\n--- 処理後の x_test の欠損値 ---\")\n# 欠損値が0になっていることを確認\nprint(test['BsmtFullBath'].isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. 学習データ(x_train)の最頻値を計算する\n# .mode()は最頻値をSeriesとして返すため、[0]で最初の値(最も頻度が高い値)を取得\nmode_value = test['BsmtHalfBath'].mode()[0]\n\n# 2. 計算した最頻値で、テストデータ(x_test)の欠損値を埋める\n# .fillna() を使います。inplace=Trueで元のDataFrameを直接変更します。\ntest['BsmtHalfBath'].fillna(mode_value, inplace=True)\n\nprint(f\"\\n学習データの'BsmtHalfBath'の最頻値は: {mode_value}\")\n\n# print(\"\\n--- 処理後の test ---\")\n# print(test['BsmtFullBath'])\n\nprint(\"\\n--- 処理後の x_test の欠損値 ---\")\n# 欠損値が0になっていることを確認\nprint(test['BsmtHalfBath'].isna().sum())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train['TotalBath'] = train_df['FullBath'] + (0.5 * train_df['HalfBath']) + \\\n                  train_df['BsmtFullBath'] + (0.5 * train_df['BsmtHalfBath'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test['TotalBath'] = test['FullBath'] + (0.5 * test['HalfBath']) + \\\n                  test['BsmtFullBath'] + (0.5 * test['BsmtHalfBath'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['TotalBath'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['TotalBath'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" TotalBathの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # testデータの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_test['TotalBath'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" TotalBathの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qual_series = x_test['TotalBath']\n\n# # 平均値と標準偏差を計算\n# mean_val = qual_series.mean()\n# std_val = qual_series.std()\n\n\n# # --- ばらつきをグラフで確認 ---\n# # グラフを2つ並べるための設定\n# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n# fig.suptitle('TotalBath of Variability', fontsize=16)\n\n\n# # 1. 左側のグラフ: ヒストグラム\n# sns.histplot(qual_series, kde=True, ax=axes[0], bins=10, discrete=True)\n# axes[0].set_title('Distribution')\n# axes[0].set_xlabel('TotalBath')\n# axes[0].set_ylabel('Count')\n# # 平均値の場所に縦線を引く\n# axes[0].axvline(mean_val, color='red', linestyle='--', label=f'mean: {mean_val:.2f}')\n# axes[0].legend()\n\n\n# # 2. 右側のグラフ: 箱ひげ図\n# sns.boxplot(y=qual_series, ax=axes[1])\n# axes[1].set_title('Variability')\n# axes[1].set_ylabel('TotalBath')\n\n\n# # グラフを表示\n# plt.tight_layout(rect=[0, 0, 1, 0.96]) # suptitleと重ならないように調整\n# plt.show()\n\n\n# # --- 参考：統計情報の表示 ---\n# print(\"\\n--- 基本的な統計情報 (describe) ---\")\n# # .describe() を使うと、平均、標準偏差、最小値、最大値などを一覧で確認できます。\n# print(qual_series.describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"drop_cols = ['FullBath']\n\n# x_trainとx_testの両方から削除\nx_train.drop(drop_cols, axis=1, inplace=True, errors='ignore')\nx_test.drop(drop_cols, axis=1, inplace=True, errors='ignore')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Overall_and_GrLiv= OverallQual_x_GrLivArea の追加","metadata":{}},{"cell_type":"code","source":"# cols_to_check = ['OverallQual', 'GrLivArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = train_df[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = train_df[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual', 'GrLivArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual', 'GrLivArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = train_df[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual', 'GrLivArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train['OverallQual_x_GrLivArea'] = train_df['OverallQual'] * train_df['GrLivArea']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test['OverallQual_x_GrLivArea'] = test['OverallQual'] * test['GrLivArea']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual_x_GrLivArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_train[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual_x_GrLivArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['OverallQual_x_GrLivArea'] = scaler.fit_transform(x_train[['OverallQual_x_GrLivArea']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['OverallQual_x_GrLivArea'] = scaler.transform(x_test[['OverallQual_x_GrLivArea']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual_x_GrLivArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_train[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_train[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual_x_GrLivArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['OverallQual_x_GrLivArea'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['OverallQual_x_GrLivArea'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" OverallQual_x_GrLivAreaの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # testデータの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_test['OverallQual_x_GrLivArea'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" OverallQual_x_GrLivAreaの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TotalPorchSF = OpenPorchSF + EnclosedPorch + 3SsnPorch + ScreenPorch の作成","metadata":{}},{"cell_type":"code","source":"# cols_to_check = ['OpenPorchSF', 'EnclosedPorch' , '3SsnPorch' , 'ScreenPorch']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = train_df[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = train_df[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OpenPorchSF', 'EnclosedPorch' , '3SsnPorch' , 'ScreenPorch']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OpenPorchSF', 'EnclosedPorch' , '3SsnPorch' , 'ScreenPorch']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = train_df[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OpenPorchSF', 'EnclosedPorch' , '3SsnPorch' , 'ScreenPorch']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train['TotalPorchSF'] = train_df['OpenPorchSF'] + train_df['EnclosedPorch'] + train_df['3SsnPorch'] + train_df['ScreenPorch']\n# x_test['TotalPorchSF'] = test['OpenPorchSF'] + test['EnclosedPorch'] + test['3SsnPorch'] + test['ScreenPorch']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ##参考\n\n# # スケーラーのインスタンスを作成\n# scaler = RobustScaler()\n\n# # 1. 学習データに対して fit + transform（学習＋変換）\n# x_train['TotalPorchSF'] = scaler.fit_transform(x_train[['TotalPorchSF']])\n\n# # 2. テストデータに対して transform のみ（同じスケール基準で変換）\n# x_test['TotalPorchSF'] = scaler.transform(x_test[['TotalPorchSF']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #データフレームから直接計算\n# # データをDataFrameとして読み込む\n# df_1 = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n\n# # 【正しいコード】\n# # df['列名'] の形は pandas Series なので、.corr() が使えます\n# correlation = x_train['TotalPorchSF'].corr(df_1['SalePrice'])\n\n# print(\"CSV→上記でx_trainに追加したデータフレームで直接計算した相関係数:\")\n# print(correlation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 学習用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_train['TotalPorchSF'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" TotalPorchSFの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # test用データの歪度を調べる\n# # --- 1. 'FullBath' の歪度を計算 ---\n# skewness = x_test['TotalPorchSF'].skew()\n\n\n# # --- 2. 結果の表示 ---\n# print(f\" TotalPorchSFの歪度(学習用): {skewness:.4f}\")\n# print(\"-----------------------------------------\")\n\n\n# # --- 歪度の解釈 ---\n# print(\"【歪度の目安】\")\n# if -0.5 <= skewness <= 0.5:\n#     print(\" -> ほぼ左右対称な分布で対数変換は不要の可能性が大です。\")\n# elif skewness > 0.5:\n#     print(\" -> 右に裾が長い（右に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")\n# else: # skewness < -0.5\n#     print(\" -> 左に裾が長い（左に歪んだ）分布です。データの種類と性質に応じて対数変換をお勧めします。\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'TotalPorchSF' in df.columns:\n#         df.drop(columns='TotalPorchSF', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":" # df['HasGarage'] = (df['GarageArea'] > 0).astype(int)　の追加　ガレージあるなし","metadata":{}},{"cell_type":"code","source":"# cols_to_check = ['GarageArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_train[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_train[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['GarageArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['GarageArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_train[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['GarageArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train['HasGarage'] = (train_df['GarageArea'] > 0).astype(int)\n# x_test['HasGarage'] = (test['GarageArea'] > 0).astype(int) ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'HasGarage' in df.columns:\n#         df.drop(columns='HasGarage', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# プールありなし　の追加（テスト、学習　ともに欠損値なし、共にint）","metadata":{}},{"cell_type":"code","source":"# cols_to_check = ['PoolArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = train_df[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = train_df[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['PoolArea']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['PoolArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = train_df[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['PoolArea']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train['HasPool'] = (train_df['PoolArea'] > 0).astype(int)\n# x_test['HasPool'] = (test['PoolArea'] > 0).astype(int)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'HasPool' in df.columns:\n#         df.drop(columns='HasPool', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neighborhood_MedianPrice の追加　欠損値は中央値で補完ずみ。","metadata":{}},{"cell_type":"code","source":"# 1. 【学習データのみ】で、NeighborhoodごとのSalePriceの中央値を計算\n#    .groupby()でグループ化し、.median()で中央値を計算\n#    .to_dict()で {'地域名': 価格} の辞書形式に変換\nneighborhood_price_map = train_df.groupby('Neighborhood')['SalePrice'].median().to_dict()\n\nprint(\"\\n--- 学習データから計算した地域ごとの価格中央値 ---\")\nprint(neighborhood_price_map)\n\n# 2. 【学習データとテストデータ】の両方に、計算した中央値をマージ（適用）する\n#    .map() メソッドを使うのが最も簡単です。\nx_train['Neighborhood_MedianPrice'] = train_df['Neighborhood'].map(neighborhood_price_map)\nx_test['Neighborhood_MedianPrice'] = test['Neighborhood'].map(neighborhood_price_map)\n\n# 3. テストデータにしか存在しない地域がある場合の欠損値処理\n#    テストデータに'Gilbert'や'StoneBr'のように学習データにない地域があった場合、\n#    .map()の結果はNaNになる。これを全体の価格の中央値などで埋めるのが安全。\noverall_median_price = train_df['SalePrice'].median()\nx_test['Neighborhood_MedianPrice'].fillna(overall_median_price, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"##参考\n\n# スケーラーのインスタンスを作成\nscaler = RobustScaler()\n\n# 1. 学習データに対して fit + transform（学習＋変換）\nx_train['Neighborhood_MedianPrice'] = scaler.fit_transform(x_train[['Neighborhood_MedianPrice']])\n\n# 2. テストデータに対して transform のみ（同じスケール基準で変換）\nx_test['Neighborhood_MedianPrice'] = scaler.transform(x_test[['Neighborhood_MedianPrice']])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Neighborhood_MedianPrice']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_train[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_train[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Neighborhood_MedianPrice']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Neighborhood_MedianPrice']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Neighborhood_Richの追加　テスト、学習　欠損値なし　ともにint","metadata":{}},{"cell_type":"code","source":"# # 1. 【学習データのみ】で、NeighborhoodごとのSalePriceの中央値を計算\n# #    .groupby()でグループ化し、.median()で中央値を計算\n# #    .to_dict()で {'地域名': 価格} の辞書形式に変換\n# neighborhood_price_map = train_df.groupby('Neighborhood')['SalePrice'].median().to_dict()\n\n# print(\"\\n--- 学習データから計算した地域ごとの価格中央値 ---\")\n# print(neighborhood_price_map)\n\n# # 2. 【学習データとテストデータ】の両方に、計算した中央値をマージ（適用）する\n# #    .map() メソッドを使うのが最も簡単です。\n# x_train['Neighborhood_MedianPrice2'] = train_df['Neighborhood'].map(neighborhood_price_map)\n# x_test['Neighborhood_MedianPrice2'] = test['Neighborhood'].map(neighborhood_price_map)\n\n# # 3. テストデータにしか存在しない地域がある場合の欠損値処理\n# #    テストデータに'Gilbert'や'StoneBr'のように学習データにない地域があった場合、\n# #    .map()の結果はNaNになる。これを全体の価格の中央値などで埋めるのが安全。\n# overall_median_price = train_df['SalePrice'].median()\n# x_test['Neighborhood_MedianPrice2'].fillna(overall_median_price, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 1. 【学習データのみ】を使って、上位25%の閾値を計算する\n# #    .quantile(0.75) で75パーセンタイル（上位25%の境界値）を計算\n# rich_threshold = x_train['Neighborhood_MedianPrice2'].quantile(0.90)\n\n# print(f\"\\n--- 学習データから計算した「高級」と判断する価格の閾値: ${rich_threshold:,.0f} ---\")\n\n# # 2. 【学習データとテストデータ】の両方に、計算した閾値を適用してフラグを作成\n# #    .apply()とlambda関数を使うと、条件分岐を簡潔に書けます\n# for df in [x_train, x_test]:\n#     df['Neighborhood_Rich'] = df['Neighborhood_MedianPrice2'].apply(\n#         lambda x: 1 if x >= rich_threshold else 0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'Neighborhood_MedianPrice2' in df.columns:\n#         df.drop(columns='Neighborhood_MedianPrice2', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Neighborhood_Rich']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Neighborhood_Rich']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Rich_x_Qual の追加  テスト及び学習ともに欠損値なし。ともにint","metadata":{}},{"cell_type":"code","source":"# # 処理を共通化するために、DataFrameをリストにまとめる\n# all_dfs = [x_train, x_test]\n# for df in all_dfs:\n# # 新しいカラム名は何でも良いですが、分かりやすい名前をつけます\n#     df['Rich_x_Qual'] = df['Neighborhood_Rich'] * df['OverallQual']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Rich_x_Qual']\n\n# print(\"--- 各カラムのデータ型とユニークな値 ---\")\n# for col_name in cols_to_check:\n#     print(\"-----------------------------------------\")\n#     print(f\"カラム名: '{col_name}'\")\n    \n#     # 1. データ型 (dtype) を表示\n#     dtype = x_test[col_name].dtype\n#     print(f\"  データ型: {dtype}\")\n    \n#     # 2. ユニークな値 (unique values) を表示\n#     unique_values = x_test[col_name].unique()\n    \n#     # ユニークな値が多すぎる場合(例: 15個以上)は、件数だけ表示するように工夫\n#     if len(unique_values) > 15:\n#         print(f\"  ユニークな値の数: {len(unique_values)} 個\")\n#     else:\n#         print(f\"  ユニークな値: {unique_values}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['Rich_x_Qual']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_test[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'Neighborhood_Rich' in df.columns:\n#         df.drop(columns='Neighborhood_Rich', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'OverallQual' in df.columns:\n#         df.drop(columns='OverallQual', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for df in [x_train, x_test]:\n#     if 'Rich_x_Qual' in df.columns:\n#         df.drop(columns='Rich_x_Qual', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# OverallQual_x_TotalSFの追加　＊＊TotalSFが標準化されている状態で作成だが、なぜかこちらのほうが精度がよかったため保管\n","metadata":{}},{"cell_type":"code","source":"# --- ここが OverallQual_x_TotalSF を追加するコード ---\n\n# 処理を共通化するために、x_trainとx_testをリストにまとめる\nall_dfs = [x_train, x_test]\n\n# forループを使って、リスト内の各DataFrameに同じ処理を適用\nfor df in all_dfs:\n    # 'OverallQual_x_TotalSF' という新しいカラムを作成し、\n    # 'OverallQual' 列と 'TotalSF' 列の掛け算の結果を代入する\n    df['OverallQual_x_TotalSF'] = df['OverallQual'] * df['TotalSF']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# cols_to_check = ['OverallQual_x_TotalSF']\n\n# print(\"--- 各カラムの欠損値件数（ループ処理） ---\")\n# for col_name in cols_to_check:\n#     # あなたのコードをループ内で使用\n#     missing_count = x_train[col_name].isna().sum()\n#     print(f\"'{col_name}' の欠損値件数: {missing_count} 件\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.drop('TotalBsmtSF2', axis=1, inplace=True)\nx_test.drop('TotalBsmtSF2', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.drop('TotalBsmtSF3', axis=1, inplace=True)\nx_test.drop('TotalBsmtSF3', axis=1, inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outlier_Flag をカラムから削除","metadata":{}},{"cell_type":"code","source":"for df in [x_train, x_test]:\n    if 'Outlier_Flag' in df.columns:\n        df.drop(columns='Outlier_Flag', inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 処理後のデータフレーム全体の確認","metadata":{}},{"cell_type":"code","source":"# #pandas のデフォルト表示幅制限を解除↓\n# #学習用データの種類数 現状\n# summary = pd.DataFrame({\n#     '種類数': [x_train[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_train[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n# }, index=features)\n\n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #pandas のデフォルト表示幅制限を解除↓\n# #テスト用データの種類数 現状\n# summary = pd.DataFrame({\n#     '種類数': [x_test[col].nunique(dropna=True) for col in features],\n#     'サンプル値': [x_test[col].dropna().unique()[:13] for col in features]  # 最大5種表示\n# }, index=features)\n\n# with pd.option_context('display.max_colwidth', None):\n#     display(summary)\n\n\n# summary","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_train_backup.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_train.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# x_test_backup.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x_test.dtypes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## バックアップ","metadata":{}},{"cell_type":"markdown","source":"## 特徴量処理進捗","metadata":{}},{"cell_type":"code","source":"#１ #'SaleCondition',[Normal, Partial, Abnorml, Family, Alloca, AdjLandであり、価値区分をつけて 　@Normal=0 Abnormal=1 でマッピング済\n#２ #'SaleType',[WD, COD, New, ConLD, Oth, Con, ConLw, ConLI, CWD]であり、価値区分をつけて 　　　　　　　＠マッピング済\n#３ #'Functional',[Typ, Min2, Min1, Mod, Maj1, Sev, Maj2]であり、価値区分をつけて 　　　　　　　　　　　　＠マッピング済\n#４ # 'MSZoning',[RH, RL, RM, FV, C (all)]であり、価値区分をつけて 　　　　　　　　　　　　　　　　　　　　　＠マッピング済\n#５ # 'Neighborhood',[NAmes, Gilbert, StoneBr, BrDale, NPkVill, のような文字列なので、価値区分をつけて　　@マッピング済\n#６ #'LotArea',[11622, 14267, 13830, 9978, 5005, 10000, 7980, 8402, 10176, 8400, 5858, 1680,等であり、＠標準化済(RobustScaler)\n#７ #'MSSubClass',　20, 60, 120, 160, 80, 30, 50, 90, 85, 190, 45, 70, 75のコード表記であるため、価値区分をつけて　　@マッピング済\n#８ #'TotRmsAbvGrd',地上階の部屋数であり、バスルームと地下室は含まない。多ければ概ね高評価だが、＠＠家の総面積あたりに対して部屋数が多すぎると逆効果　@そのまま 行うなら面積／部屋数 のような実居住性の特徴量を追加\n#９ #'GrLivArea',[896, 1329, 1629, 1604, 1280, 1655, 1187, 1465, 1341, 882, 1337, 987, 109等であり、＠標準化済（RobustScaler)\n#１０ #'1stFlrSF',896, 1329, 928, 926, 1280, 763, 1187, 789, 1341, 882, 1337, 483, 525等であり、　　 ＠標準化済（RobustScaler)\n#１１ #'BsmtQual',　　＠マッピング済\n#１２ #'BsmtCond',　　　＠マッピング済\n#１３ #'BsmtExposure',  ＠マッピング済\n#１４ #'BsmtFinType1',Rec, ALQ, GLQ, Unf, BLQ, LwQであり、　　　　　　　　　　　　　　　　　　　　　　　　　　　　　　＠マッピング済み（欠損値が最初より増えたので留意）\n#１５ #'BsmtFinSF1',468.0, 923.0, 791.0, 602.0, 263.0, 0.0, 935.0, 637.0, 804.0, 1051.0, 156.0, 300.0, 514.0などで。＠→標準化済（RobustScaler)\n#１６ #'Heating',　　＠マッピング済　⇨ ８月１０日特徴量から削除\n#１７ #'HeatingQC',＠マッピング済\n#１８ #'CentralAir',マッピング済\n#１９ #'KitchenAbvGr',＠マッピング済\n#２０ #'KitchenQual',＠マッピング済\n#２１ #'ExterQual',＠マッピング済\n#２２ #'GarageCars',＠数字がそのまま価値をもつためマッピング不要[1.0, 2.0, 3.0, 0.0, 4.0, 5.0]の台数であり、概ね台数が多ければ評価はあがる。＠マッピング済\n#２３ #'GarageArea']730.0, 312.0, 482.0, 470.0, 506.0, 440.0, 42等であり、　　　　　　　　　　　　　　　＠標準化済（RobustScaler)\n#23 #OvalQUAL 0810追加","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 目的変数と説明変数の分割","metadata":{}},{"cell_type":"markdown","source":"## バリデーション","metadata":{}},{"cell_type":"markdown","source":"### ** クロスバリデーションでの分割方法","metadata":{}},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split, KFold を一番上記のコードに入れる。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#　パラメータのチューニング","metadata":{}},{"cell_type":"code","source":"# 回帰、評価指標RMSEの場合 のパラメータの設定をする。gbdt\n#、n_estimatorsはアンサンブルモデルの複雑さと学習量を直接コントロールする非常に重要なパラメータです。 特に勾配ブースティングでは、\n#この値を適切に（Early Stoppingを使って）管理することが、過学習を防ぎ\n#n_estimators は歩数　learning_rate は歩幅こちらはトレードオフであり、learning_rate＞n_estimatorsは不自然な場合が多いと予想\n\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'rmse',\n    'learning_rate': 0.1,\n    'num_leaves': 16,\n    'n_estimators': 100000,\n    \"random_state\": 123,\n    \"importance_type\": \"gain\",\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# グリッドサーチ（ここから）","metadata":{}},{"cell_type":"code","source":"# param_grid = {\n#     'learning_rate': [0.001，0.1, 1],\n#     'num_leaves': [31, 16, 50],\n#     'n_estimators': [50000, 100000, 200000]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = lgb.LGBMRegressor(random_state=123)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# # X: train_df から SalePrice を除いた特徴量\n# # y: train_df の SalePrice\n\n# # 学習データ全体を、訓練用(x_train, y_train)と検証用(x_val, y_val)に分割\n# # 例えば、80%を訓練に、20%を検証に使う\n# # random_stateを固定すると、毎回同じように分割される\n# x_train, x_val, y_train, y_val = train_test_split(\n#     X, y, test_size=0.2, random_state=123\n# )\n\n# # これで作成された x_train, y_train を grid_search.fit() に渡し、\n# # x_val, y_val を fit_params の eval_set に渡す","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# grid_search = GridSearchCV(model, param_grid, cv=5)\n# grid_search.fit(x_train, y_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_params = grid_search.best_params_\n# print(\"最適なパラメータ:\", best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## バリデーション","metadata":{}},{"cell_type":"code","source":"#n_splitsの変数 に５を事前に入れている。\nn_splits = 5\n\n#StratifiedKFoldを使って、n_splitsを、上記で設定していた n_splits=5 回分割するという意味。#cv は任意であるが、Cross　Validation の頭文字からとっている。\ncv = KFold(n_splits=n_splits, shuffle=True, random_state=123)\n\nmetrics = []\nimp = pd.DataFrame()\n\n#train_idx, val_idxを事前に準備しておき、\n#5分割したデータセットのインデックスを毎回のループごとに受け取れるようにした。↓\n#for train_idx, val_idx in (cv.split(x_train, y_train):\n#また、何番目の分割かを知りたいので、enumerateを　cv.split(x_train, y_train)に行い、\n#追加して、インデックスも取得したいので、nfold, も (train_idx, val_idx)に使う。\n\nfor nfold, (train_idx, val_idx) in enumerate(cv.split(x_train)):\n    print(\"-\"*10, nfold, \"-\"*10)\n\n#下記コードで、８対２（train_df内で、学習用８割、評価用２割の意味）学習用の分割にインデックスなしでデータが５分割されていることがわかる。\n#本番用のtestデータはここでは分割されていない。   \n    #print(val_idx[:10])\n    \n#ここまでかけたら、split で学習用（説明変数・目的変数）と評価用（説明変数と目的変数）を\n#学習用の説明変数インデックスと学習用の目的変数に対して、iloc で（行番号、列番号（ここでは２種類の説明変数））と、\n#目的変数のインデックスを抽出し、x_tr と y_trに入れてあげる。(＊評価用の説明変数と目的変数のインデックスに対しても同じことをしている）\n\n    \n    x_tr, y_tr = x_train.iloc[train_idx], y_train.iloc[train_idx]\n    x_va, y_va = x_train.iloc[val_idx], y_train.iloc[val_idx]\n\n    \n    print(\"train_df 内の学習用説明変数データ:\", x_tr.shape)\n    print(\"train_df 内の評価用説明変数データ:\", x_va.shape)\n\n    print(\"train_df 内の学習用目的変数データ:\", y_tr.shape)\n    print(\"train_df 内の評価用目的変数データ:\", y_va.shape)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## モデル学習","metadata":{}},{"cell_type":"code","source":"#lightGBM を選ぶ理由\n#モデルの精度が高い、処理が高速、カテゴリ変数を数値に変換しなくても処理できる（*objectはカテゴリーに要変換)\n#欠損値があっても学習をおこなえる、外れ値の影響を受けずらい。などがある。\n\n#パラメータの説明\n\n#eval_set\t上記「評価指標」算出のためのデータセットを指定する引数（ここでは、デフォルトの値なので、数の指定はしていない）\n#目的としては、主に学習過程を監視し、過学習を防ぐための早期停止を行うため等がある。\n#LGBMRegressor は boosting_type パラメータでブースティング方法を切り替えられます：\n\n#boosting_type の値\tモデルの種類\t説明\n#'gbdt'（デフォルト）\t勾配ブースティング決定木\t最も一般的で高精度\n#'dart'\tDropouts meet Multiple Additive Regression Trees\tGBDTにランダム性を加えて過学習を防ぐ\n#'goss'\tGradient-based One-Side Sampling\tGBDTを高速化（低メモリ・高速）\n#'rf'\tランダムフォレスト風ブースティング\tランダム性のある木構造（GBDTではない）\n\n#python\n\n#model = lgb.LGBMRegressor(boosting_type='dart')  # 例: DARTに変更\n\n\n# params['boosting_type'] = 'dart'\nmodel = lgb.LGBMRegressor(**params)\n\n\nmodel.fit(\n        x_tr,\n        y_tr,\n        eval_set=[(x_tr, y_tr), (x_va, y_va)],\n        callbacks=[\n            lgb.early_stopping(stopping_rounds=100, verbose=True),\n            lgb.log_evaluation(0)\n        ],\n    )\n#lighGBMはobject型は扱えないので、astypeメソッドで、category型に変換する必要がある。","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 学習したモデルでの推論・計算","metadata":{}},{"cell_type":"code","source":"#精度を計算するには、モデルの推論・予測値を先に計算する必要がある。\n#下記は、モデルによる学習用データを使った目的変数への精度と、モデルによる評価用データを使った目的変数への精度を計算している。\n#model.predictを使ってモデルの予測値を計算して、そのあとに正解と予測値を使って正解率を計算する。\ny_tr_pred = model.predict(x_tr)\ny_va_pred = model.predict(x_va)\n\n#上記でモデルの予測値の計算が終わったので、下記で、RMSEの計算を、目的変数（学習用データを使って）と目的変数（評価用データを使って）を使って行う。\nrmse_tr = np.sqrt(mean_squared_error(y_tr, y_tr_pred))\nrmse_va = np.sqrt(mean_squared_error(y_va, y_va_pred))\n\n#format 関数で、rmse_tr とrmse_va の値を ｛｝内に、小数点第2位までプリントしている。＊少数点第２位（:.2f)\nprint(\"[RMSE] tr: {:.2f}, va: {:.2f}\".format(rmse_tr, rmse_va))\n\n#metrics変数に appendで　nfold, rmse_tr, rmse_va　を追加してあげる。\nmetrics.append([nfold, rmse_tr, rmse_va])\n\n\n_imp = pd.DataFrame({\n    \"col\": x_train.columns,\n    \"imp\": model.feature_importances_,\n    \"nfold\": nfold\n})\n#concatメソッドで、事前に結合したいDataframeの変数を2つ以上用意し[ ]リスト形式で記載している。\n#なお、axis=0 は（デフォルトで）縦に、axis=1 は横にデータを連結する。\n#引数ignore_index=Trueを指定すると、DataFrameを連結後、連結の軸方向のインデックスを無視して0から値を振り直します。(デフォルトはFalse)\nimp = pd.concat([imp, _imp], axis=0, ignore_index=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# np.arrayの応用についてにはこちら→　https://www.choge-blog.com/programming/python-numpy-array/\n\n#ここは単純に、一次元標記のため、array を使用している。\nmetrics = np.array(metrics)\n#すでに指定した metrics 内　の値である、「fold 回数」、「平均」、「標準偏差」 がプリントされている。\n#上記は、[[4.0000000e+00 4.7772013e+04 5.1268212e+04]]の部分である。\nprint(metrics)\n\n#tr: {:.2f} に metrics[:,1].mean()の値を　 va: {:.2f}にmetrics[:,1].std()の値を入れている。\n#[cv ] tr: +-{:.2f}に、metrics[:,2].mean()の値を　 va: +-{:.2f}に　metrics[:,2].std()の値と入れている。\nprint(\"[cv ] tr: {:.2f}+-{:.2f}, va: {:.2f}+-{:.2f}\".format(\n  metrics[:,1].mean(), metrics[:,1].std(),\n  metrics[:,2].mean(), metrics[:,2].std(),\n))\n   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# groupbyでcolをグルーピング（ここでは、すでに選択した8種類）し、\n#imp = pd.concat([imp, _imp], axis=0, ignore_index=True) で作成したデータフレームの imp の平均値(mean)と標準偏差(std)をaggで計算している。\n#なお、agg はaggregate の意味で　「集約する」　という意味がある。\nimp = imp.groupby(\"col\")[\"imp\"].agg([\"mean\", \"std\"])\n\n#imp のデータフレームにカラムを追加して、上記の imp に対して agg で計算した　mean と　std　の値を入れている。\nimp.columns = [\"imp\", \"imp_std\"]\n\n#sort_values(by='imp' は、imp列を基準に並び変えるという意味。\n#ascending=False は、データを昇順に並べる意味。Trure であれば降順という意味\nimp_df = imp.sort_values(by='imp', ascending=False)\nimp_df.head(30)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#特徴量の重要度を可視化させている。なお、もってくる特徴量の変数は、モデルの計算用に使用した、x_tr(テストデータ用の説明変数である)\n#なお、MSsubclassは 数値型であるが、実質的にはカテゴリ変数のため、その数字に意味を持たないので、重要度が低くなっている。（必ず変換して意味を持たせる必要あり）\nimp_df = pd.DataFrame({\n    \"features\": x_tr.columns,\n    \"imp\": model.feature_importances_\n})\nimp_df = imp_df.sort_values(by=\"imp\", ascending=False)\n\nplt.figure(figsize=(14, 6))\nsns.barplot(x=\"imp\", y=\"features\", data=imp_df.head(30))\nplt.title(\"Feature Importances\")\nplt.xlabel(\"Importance\")\nplt.ylabel(\"Featurre\")\n\nplt.xticks(fontsize=9)\nplt.yticks(fontsize=9)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **提出用のデータフレームに整形する","metadata":{}},{"cell_type":"code","source":"#作成したモデルで、x_test を 計算（model.predict(x_test) の部分）し、これを  y_test_pred にいれている。\ny_test_pred = model.predict(x_test)\n\ndf_submit = pd.DataFrame({\n    'Id': test.Id,\n    'SalePrice': y_test_pred\n})\ndf_submit.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **CSVファイルを出力する。","metadata":{}},{"cell_type":"code","source":"df_submit.to_csv(\"hp-submission_toriaeze20250912.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}